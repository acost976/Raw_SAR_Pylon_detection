{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acost976/Raw_SAR_Pylon_detection/blob/main/Classifier_IAR_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SBOjgIypcGaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ff39f5-7583-406a-cac2-0db116e2dfdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim as optim\n",
        "import torch.utils.data.dataloader as dataloader\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torchvision:\", torchvision.__version__)\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "emCZen_vmsx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00652d65-49fa-4046-d5c7-0a02552b1ed3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.8.0+cu126\n",
            "torchvision: 0.23.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe ProcessedSARImageDatasetlegge immagini .png da directory strutturate come:\n",
        "\n",
        "\n",
        "```\n",
        "root_dir/\n",
        "  Integri/\n",
        "      Integri_subfolder1/\n",
        "      Integri_subfolder2/\n",
        "  Rimossi/\n",
        "      Rimossi_subfolder1/\n",
        "  Abbattuti/\n",
        "      Abbattuti_subfolder1/\n",
        "```\n",
        "\n",
        "\n",
        "Le immagini sono in formato .png a 3 canali, dove il primo rappresenta il valore reale, il secondo il valore complesso e il terzo canale vuoto\n",
        "\n",
        "Permette inoltre di:\n",
        "1. selezionare specifiche classi o sottocartelle,\n",
        "2. applicare trasformazioni torchvision,\n",
        "3. opzionalmente mescolare le etichette (in caso si volesse fare un sanity check)."
      ],
      "metadata": {
        "id": "2Clc2grWp74C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ro4eQh4rWrGD"
      },
      "outputs": [],
      "source": [
        "class ProcessedSARImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset personalizzato per immagini SAR (Synthetic Aperture Radar) processate,\n",
        "    organizzate in sottocartelle per classe.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root_dir : str\n",
        "        Percorso alla directory principale che contiene le classi.\n",
        "    transform : callable, optional\n",
        "        Trasformazioni da applicare a ciascuna immagine (es. `transforms.Compose([...])`).\n",
        "    shuffle_labels : bool, default=False\n",
        "        Se True, mescola l'ordine di immagini ed etichette (utilizzato per sanity check).\n",
        "    selected_classes : list of str, optional\n",
        "        Lista di nomi di classi da includere (default: ['Integri', 'Rimossi', 'Abbattuti']).\n",
        "    selected_subfolders : list of str, optional\n",
        "        Lista di sottocartelle per ciascuna classe (default: [''] usa la root di ogni classe).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    image_paths : list of str\n",
        "        Percorsi completi alle immagini trovate.\n",
        "    labels : list of int\n",
        "        Etichette numeriche corrispondenti alle classi.\n",
        "    classes : list of str\n",
        "        Nomi delle classi considerate nel dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None, shuffle_labels=False,\n",
        "                 selected_classes=None, selected_subfolders=None):\n",
        "        \"\"\"\n",
        "        Inizializza il dataset e popola la lista di immagini ed etichette.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Define the class names and their corresponding labels\n",
        "        if selected_classes is not None:\n",
        "            self.classes = selected_classes\n",
        "        else:\n",
        "            self.classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "\n",
        "        if selected_subfolders is None:\n",
        "            selected_subfolders = ['']  # Use the root directory for each class if no subfolders are specified\n",
        "\n",
        "        for label, class_name in enumerate(self.classes):\n",
        "            for subfolder in selected_subfolders:\n",
        "                # Construct the full path to the subfolder within the class directory\n",
        "                class_dir = os.path.join(root_dir, class_name, class_name + subfolder)\n",
        "                print(f\"Processing data from: {class_dir}\")\n",
        "\n",
        "                if not os.path.isdir(class_dir):\n",
        "                    print(f\"Warning: Class subfolder not found: {class_dir}\")\n",
        "                    continue\n",
        "\n",
        "                for filename in os.listdir(class_dir):\n",
        "                    if filename.endswith('.png'):\n",
        "                        self.image_paths.append(os.path.join(class_dir, filename))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "        if shuffle_labels:\n",
        "            combined = list(zip(self.image_paths, self.labels))\n",
        "            random.shuffle(combined)\n",
        "            self.image_paths, self.labels = zip(*combined)\n",
        "            print(\"Training labels and image paths have been shuffled.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Lunghezza del dataset\"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Restituisce un'immagine e la relativa etichetta.\n",
        "        \"\"\"\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        image_tensor = transforms.ToTensor()(image)\n",
        "\n",
        "        # Convert back to PIL to allow further transforms\n",
        "        image = transforms.ToPILImage()(image_tensor)\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Ensure we only keep two channels\n",
        "        if image_tensor.shape[0] == 2:\n",
        "            image_tensor = image_tensor\n",
        "        elif image_tensor.shape[0] == 3:\n",
        "            ch3 = image_tensor[2]\n",
        "            if not torch.allclose(ch3, torch.zeros_like(ch3), atol=1e-6):\n",
        "                warnings.warn(\"Third channel is not empty\", UserWarning)\n",
        "            image_tensor = image_tensor[:2]\n",
        "        else:\n",
        "            warnings.warn(f\"Unexpected number of channels ({image_tensor.shape[0]}).\", UserWarning)\n",
        "            image_tensor = image_tensor[:2]\n",
        "\n",
        "        return image_tensor, label\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper di training per un modello PyTorch: gestisce ottimizzatore, salvataggio/caricamento checkpoint, loop di training/val/test e logging di metriche."
      ],
      "metadata": {
        "id": "q0MfYVm9shh0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MyU4xVZ9krUm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data as dataloader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class ModelTrainer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        device,\n",
        "        loss_fun,\n",
        "        batch_size,\n",
        "        learning_rate,\n",
        "        save_dir,\n",
        "        model_name,\n",
        "        start_from_checkpoint: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Inizializza il trainer e imposta parametri, modello e ottimizzatore.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : nn.Module\n",
        "            Il modello neurale vero e proprio che vogliamo allenare.\n",
        "        device : torch.device or str\n",
        "            Il device su cui eseguire il training (ad es. \"cuda\" o \"cpu\").\n",
        "        loss_fun : callable\n",
        "            Funzione di loss che accetta (predictions, targets) e ritorna uno scalar Tensor.\n",
        "        batch_size : int\n",
        "            Dimensione del batch da usare nei DataLoader.\n",
        "        learning_rate : float\n",
        "            Learning rate per l'ottimizzatore.\n",
        "        save_dir : str\n",
        "            Directory dove salvare i checkpoint.\n",
        "        model_name : str\n",
        "            Nome base del file di checkpoint (verrà creato `model_name.pt`).\n",
        "        start_from_checkpoint : bool, default=False\n",
        "            Se True, prova a caricare un checkpoint esistente e riprendere l'allenamento.\n",
        "            Se False, impedisce di sovrascrivere accidentalmente un checkpoint già esistente.\n",
        "        \"\"\"\n",
        "\n",
        "        # Call the parent nn.Module constructor\n",
        "        super(ModelTrainer, self).__init__()\n",
        "\n",
        "        # core training objects\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.loss_fun = loss_fun\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # logging / tracking\n",
        "        self.start_epoch = 0\n",
        "        self.best_valid_acc = 0.0\n",
        "\n",
        "        self.train_loss_logger = []\n",
        "        self.train_acc_logger = []\n",
        "        self.val_acc_logger = []\n",
        "        self.val_loss_logger = []\n",
        "        self.test_acc_logger = []\n",
        "        self.test_loss_logger = []\n",
        "\n",
        "        # loaders will be attached later\n",
        "        self.train_loader = None\n",
        "        self.valid_loader = None\n",
        "        self.test_loader = None\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = None\n",
        "        self.set_optimizer()\n",
        "\n",
        "        # checkpoint stuff\n",
        "        self.save_dir = save_dir\n",
        "        self.save_path = os.path.join(save_dir, model_name + \".pt\")\n",
        "\n",
        "        if not os.path.isdir(self.save_dir):\n",
        "            os.makedirs(self.save_dir)\n",
        "\n",
        "        if start_from_checkpoint:\n",
        "            # try to resume\n",
        "            self.load_checkpoint()\n",
        "        else:\n",
        "            # safety check: don't silently overwrite an existing model\n",
        "            if os.path.isfile(self.save_path):\n",
        "                raise ValueError(f\"Warning: checkpoint already exists at {self.save_path}\")\n",
        "            else:\n",
        "                print(\"New model creation\")\n",
        "\n",
        "    def set_optimizer(self):\n",
        "        # IMPORTANT: optimize the wrapped model's parameters, not the trainer's parameters()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        # Check if checkpoint exists\n",
        "        if os.path.isfile(self.save_path):\n",
        "            checkpoint = torch.load(self.save_path, map_location=self.device)\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.set_optimizer()\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "            self.start_epoch = checkpoint['epoch']\n",
        "            self.best_valid_acc = checkpoint['best_valid_acc']\n",
        "\n",
        "            self.train_loss_logger = checkpoint.get('train_loss_logger', [])\n",
        "            self.train_acc_logger = checkpoint.get('train_acc_logger', [])\n",
        "            self.val_acc_logger = checkpoint.get('val_acc_logger', [])\n",
        "            self.val_loss_logger = checkpoint.get('val_loss_logger', [])\n",
        "            self.test_acc_logger = checkpoint.get('test_acc_logger', [])\n",
        "            self.test_loss_logger = checkpoint.get('test_loss_logger', [])\n",
        "\n",
        "            print(f\"Checkpoint loaded, starting from epoch: {self.start_epoch}\")\n",
        "        else:\n",
        "            # Raise Error if it does not exist\n",
        "            raise ValueError(\"Checkpoint does not exist\")\n",
        "\n",
        "    def save_checkpoint(self, epoch, valid_acc):\n",
        "        self.best_valid_acc = valid_acc\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'best_valid_acc': valid_acc,\n",
        "            'train_loss_logger': self.train_loss_logger,\n",
        "            'train_acc_logger': self.train_acc_logger,\n",
        "            'val_acc_logger': self.val_acc_logger,\n",
        "            'val_loss_logger': self.val_loss_logger,\n",
        "            'test_acc_logger': self.test_acc_logger,\n",
        "            'test_loss_logger': self.test_loss_logger,\n",
        "        }, self.save_path)\n",
        "\n",
        "    def set_data(self, train_set, val_set, test_set=None):\n",
        "        print(f'Number of training examples: {len(train_set)}')\n",
        "        print(f'Number of validation examples: {len(val_set)}')\n",
        "        if test_set is not None:\n",
        "            print(f'Number of test examples: {len(test_set)}')\n",
        "\n",
        "        self.train_loader = dataloader.DataLoader(\n",
        "            train_set,\n",
        "            shuffle=True,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=4\n",
        "        )\n",
        "        self.valid_loader = dataloader.DataLoader(\n",
        "            val_set,\n",
        "            shuffle=False,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=4\n",
        "        )\n",
        "        if test_set is not None:\n",
        "            self.test_loader = dataloader.DataLoader(\n",
        "                test_set,\n",
        "                shuffle=False,\n",
        "                batch_size=self.batch_size,\n",
        "                num_workers=4\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # delegate to the actual model\n",
        "        return self.model(x)\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"\n",
        "        Esegue una singola epoca di addestramento.\n",
        "\n",
        "        Scorre tutto il train_loader e:\n",
        "        - calcola le predizioni\n",
        "        - computa la loss\n",
        "        - fa il backward\n",
        "        - aggiorna i pesi\n",
        "        \"\"\"\n",
        "        if self.train_loader is None:\n",
        "            raise ValueError(\"Dataset not defined! Call set_data() first.\")\n",
        "\n",
        "        # Set model in train mode\n",
        "        self.model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for x, y in tqdm(self.train_loader, leave=False, desc=\"Training\"):\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            fx = self.model(x)\n",
        "\n",
        "            # Loss\n",
        "            loss = self.loss_fun(fx, y)\n",
        "\n",
        "            # Zero grad\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "\n",
        "            # Step\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Logging\n",
        "            batch_size_now = x.size(0)\n",
        "            running_loss += loss.item() * batch_size_now\n",
        "            running_correct += (fx.argmax(1) == y).sum().item()\n",
        "            total_samples += batch_size_now\n",
        "\n",
        "        epoch_loss = running_loss / total_samples\n",
        "        epoch_acc = running_correct / total_samples\n",
        "\n",
        "        self.train_loss_logger.append(epoch_loss)\n",
        "        self.train_acc_logger.append(epoch_acc)\n",
        "\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def evaluate_model(self, train_test_val=\"val\"):\n",
        "        \"\"\"\n",
        "        Valuta il modello su un dataset (train/val/test) senza aggiornare i pesi.\n",
        "        Ritorna l'accuracy media.\n",
        "        \"\"\"\n",
        "        if train_test_val == \"train\":\n",
        "            loader = self.train_loader\n",
        "            state = \"Evaluating Train Set\"\n",
        "        elif train_test_val == \"val\":\n",
        "            loader = self.valid_loader\n",
        "            state = \"Evaluating Validation Set\"\n",
        "        elif train_test_val == \"test\":\n",
        "            loader = self.test_loader\n",
        "            state = \"Evaluating Test Set\"\n",
        "        else:\n",
        "            raise ValueError(\"Invalid dataset, train_test_val should be 'train'/'val'/'test'\")\n",
        "\n",
        "        if loader is None:\n",
        "            raise ValueError(f\"{train_test_val} loader not defined! Did you pass that split to set_data()?\")\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        epoch_acc = 0.0\n",
        "        epoch_loss = 0.0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(loader, leave=False, desc=state):\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "\n",
        "                fx = self.model(x)\n",
        "\n",
        "                loss = self.loss_fun(fx, y)\n",
        "                bs = x.size(0)\n",
        "\n",
        "                epoch_loss += loss.item() * bs\n",
        "                total_samples += bs\n",
        "                epoch_acc += (fx.argmax(1) == y).sum().item()\n",
        "\n",
        "        avg_loss = epoch_loss / total_samples\n",
        "        avg_acc = epoch_acc / total_samples\n",
        "\n",
        "        if train_test_val == \"train\":\n",
        "            self.train_acc_logger.append(avg_acc)\n",
        "            # you normally wouldn't log train loss here because train_model already did,\n",
        "            # but you could if you wanted symmetry\n",
        "        elif train_test_val == \"val\":\n",
        "            self.val_acc_logger.append(avg_acc)\n",
        "            self.val_loss_logger.append(avg_loss)\n",
        "        elif train_test_val == \"test\":\n",
        "            self.test_acc_logger.append(avg_acc)\n",
        "            self.test_loss_logger.append(avg_loss)\n",
        "\n",
        "        return avg_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seoP7QqZCqZr"
      },
      "source": [
        "Definizione dei parametri di training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TZYsnRadlHsr"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "num_epochs = 3\n",
        "learning_rate = 1e-5\n",
        "start_epoch = 0\n",
        "best_valid_acc = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VbLkmqw0l3XB"
      },
      "outputs": [],
      "source": [
        "# Set device to GPU_indx if GPU is avaliable\n",
        "GPU_indx = 0\n",
        "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_PkzMTt0KSa"
      },
      "source": [
        "Questa cella definisce il modello da utilizzare\n",
        "\n",
        "- Architettura ResNet o Mobilenet\n",
        "- Classificatore a 2 o 3 classi (la classe Rimossi è stata creata per fare dei test, ma difficilmente nel caso reale un traliccio viene completamente rimosso - più verosimilmente viene abbattuto )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset - Questo dataset include un crop dei 512 x 512 pixel centrali dell'immagine raw\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "\n",
        "# Si può scegliere se trainare un classificatore a due o a tre classi\n",
        "two_classes = ['Integri', 'Abbattuti']\n",
        "three_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "selected_subfolders = ['_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt', '_5x25_WithExtraTgt_Cls']\n",
        "\n",
        "\n",
        "# Crea Dataset con i parametri selezzionati\n",
        "two_class_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=two_classes, transform=transforms.ToTensor(),  selected_subfolders=selected_subfolders)\n",
        "#three_class_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=three_classes, transform=transforms.ToTensor(),  selected_subfolders=selected_subfolders)\n",
        "\n",
        "\n",
        "dataset_for_training = two_class_dataset\n",
        "num_classes = len(dataset_for_training.classes)\n",
        "\n",
        "\n",
        "# Scegli architettura\n",
        "model_architecture = 'mobilenet' # Options: 'resnet18', 'resnet50', 'mobilenet'\n",
        "\n",
        "if model_architecture == 'resnet18':\n",
        "    if num_classes == 2:\n",
        "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.conv1 = nn.Conv2d(in_channels=2,\n",
        "                                out_channels=model.conv1.out_channels,\n",
        "                                kernel_size=model.conv1.kernel_size,\n",
        "                                stride=model.conv1.stride,\n",
        "                                padding=model.conv1.padding,\n",
        "                                bias=model.conv1.bias is not None\n",
        "        )\n",
        "        model.fc = nn.Linear(num_ftrs, 2)\n",
        "    elif num_classes == 3:\n",
        "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        model.conv1 = nn.Conv2d(in_channels=2,\n",
        "                                out_channels=model.conv1.out_channels,\n",
        "                                kernel_size=model.conv1.kernel_size,\n",
        "                                stride=model.conv1.stride,\n",
        "                                padding=model.conv1.padding,\n",
        "                                bias=model.conv1.bias is not None\n",
        "        )\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, 3)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported number of classes: {num_classes}\")\n",
        "\n",
        "elif model_architecture == 'resnet50':\n",
        "    if num_classes == 2:\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.conv1 = nn.Conv2d(in_channels=2,\n",
        "                                out_channels=model.conv1.out_channels,\n",
        "                                kernel_size=model.conv1.kernel_size,\n",
        "                                stride=model.conv1.stride,\n",
        "                                padding=model.conv1.padding,\n",
        "                                bias=model.conv1.bias is not None\n",
        "        )\n",
        "        model.fc = nn.Linear(num_ftrs, 2)\n",
        "    elif num_classes == 3:\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1) #\n",
        "        model.conv1 = nn.Conv2d(in_channels=2,\n",
        "                                out_channels=model.conv1.out_channels,\n",
        "                                kernel_size=model.conv1.kernel_size,\n",
        "                                stride=model.conv1.stride,\n",
        "                                padding=model.conv1.padding,\n",
        "                                bias=model.conv1.bias is not None\n",
        "        )\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, 3)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported number of classes: {num_classes}\")\n",
        "\n",
        "elif model_architecture == 'mobilenet':\n",
        "    if num_classes == 2:\n",
        "        model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
        "        model.features[0][0] = nn.Conv2d(2, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "        num_ftrs = model.classifier[-1].in_features\n",
        "        model.classifier[-1] = nn.Linear(num_ftrs, 2)\n",
        "    elif num_classes == 3:\n",
        "        model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
        "        model.features[0][0] = nn.Conv2d(2, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "        num_ftrs = model.classifier[-1].in_features\n",
        "        model.classifier[-1] = nn.Linear(num_ftrs, 3)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported number of classes: {num_classes}\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model architecture: {model_architecture}\")\n",
        "\n",
        "\n",
        "# Split training, validation, e test\n",
        "train_size = int(0.8 * len(dataset_for_training))\n",
        "val_size = int(0.1 * len(dataset_for_training))\n",
        "test_size = len(dataset_for_training) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset_for_training, [train_size, val_size, test_size])\n",
        "\n",
        "\n",
        "def compute_mean_std(dset):\n",
        "    loader = DataLoader(dset, batch_size=batch_size, shuffle=False)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        images = images.view(images.size(0), -1)  # Flatten\n",
        "        n += images.size(0)\n",
        "        mean += images.mean(1).sum().item()\n",
        "        std += images.std(1).sum().item()\n",
        "\n",
        "    mean /= n\n",
        "    std /= n\n",
        "    return mean, std\n",
        "\n",
        "mean, std = compute_mean_std(train_dataset)\n",
        "print(\"mean: \", mean)\n",
        "print(\"std: \", std)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "normalized_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,))\n",
        "])\n",
        "\n",
        "train_dataset.dataset.transform = normalized_transform\n",
        "val_dataset.dataset.transform = normalized_transform\n",
        "test_dataset.dataset.transform = normalized_transform\n",
        "\n",
        "\n",
        "print(f'Number of training examples: {len(train_dataset)}')\n",
        "print(f'Number of validation examples: {len(val_dataset)}')\n",
        "print(f'Number of test examples: {len(test_dataset)}')\n",
        "print(f'Model with {num_classes} output classes and {model_architecture} architecture created.')"
      ],
      "metadata": {
        "id": "oDTs7P9JF0zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e693b6c-2274-4fe7-c14d-ef7d0f93e213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data from: /content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/Integri/Integri_0\n",
            "Processing data from: /content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/Integri/Integri_3x20_NoExtraTgt\n",
            "Processing data from: /content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/Integri/Integri_5x25_NoExtraTgt\n",
            "Processing data from: /content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/Integri/Integri_5x25_WithExtraTgt_Cls\n",
            "Processing data from: /content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/Abbattuti/Abbattuti_0\n",
            "Processing data from: /content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/Abbattuti/Abbattuti_3x20_NoExtraTgt\n",
            "Processing data from: /content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/Abbattuti/Abbattuti_5x25_NoExtraTgt\n",
            "Processing data from: /content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/Abbattuti/Abbattuti_5x25_WithExtraTgt_Cls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date le performance molto alte del modello sia su set di training che di validazione, ho il dubbio che il dataset sintetico sia estremamente semplice. Per capire se ci sono dei pattern nelle immagini raw che il modello sta imparando, questa cella calcola la media e deviazione standard delle immagini di coascuna classe. la cella successiva calcola la differenza fra queste medie."
      ],
      "metadata": {
        "id": "XSCA6s8q2Pzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if 'train_dataset' not in globals():\n",
        "    print(\"Error: 'train_dataset' not found. Please run the cell that creates the training dataset first.\")\n",
        "else:\n",
        "    # Create a dictionary\n",
        "    images_by_class = defaultdict(list)\n",
        "\n",
        "    # Get the class names from the dataset (assuming train_dataset is a Subset)\n",
        "    # Access the original dataset's classes attribute\n",
        "    if hasattr(train_dataset, 'dataset') and hasattr(train_dataset.dataset, 'classes'):\n",
        "        class_names = train_dataset.dataset.classes\n",
        "    else:\n",
        "        # Fallback if the structure is different\n",
        "        class_names = [str(i) for i in range(len(np.unique([label for _, label in train_dataset])))]\n",
        "        print(\"Warning: Could not access class names from the original dataset. Using numerical labels.\")\n",
        "\n",
        "    # Iterate through the training dataset and group images by class\n",
        "    for img, label in train_dataset:\n",
        "        images_by_class[label].append(img)\n",
        "\n",
        "    # Iterate through each class to calculate and plot the average image and standard deviation\n",
        "    for label, images in images_by_class.items():\n",
        "        if not images:\n",
        "            print(f\"No images found for class with label {label}.\")\n",
        "            continue\n",
        "\n",
        "        # Get the class name for the current label\n",
        "        class_name = class_names[label]\n",
        "        print(f\"Processing class: {class_name}\")\n",
        "\n",
        "        # Stack images along a new dimension\n",
        "        stacked_images = torch.stack(images)\n",
        "\n",
        "        # Calculate the average image and standard deviation for each channel\n",
        "        if stacked_images.shape[1] > 0:\n",
        "            average_channel_0 = torch.mean(stacked_images[:, 0, :, :], dim=0)\n",
        "            std_channel_0 = torch.std(stacked_images[:, 0, :, :], dim=0) # Calculate standard deviation\n",
        "\n",
        "            # Determine the min and max values across both average and standard deviation for Channel 0\n",
        "            vmin_ch0 = min(average_channel_0.min().item(), std_channel_0.min().item())\n",
        "            vmax_ch0 = max(average_channel_0.max().item(), std_channel_0.max().item())\n",
        "\n",
        "\n",
        "            # Plot average and standard deviation for Channel 0 side by side\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "            im0 = axes[0].imshow(average_channel_0.numpy(), cmap='viridis', vmin=vmin_ch0, vmax=vmax_ch0)\n",
        "            axes[0].set_title(f'Average Channel 0: {class_name}')\n",
        "            axes[0].axis('off')\n",
        "            fig.colorbar(im0, ax=axes[0])\n",
        "\n",
        "\n",
        "            im1 = axes[1].imshow(std_channel_0.numpy(), cmap='viridis', vmin=vmin_ch0, vmax=vmax_ch0)\n",
        "            axes[1].set_title(f'Standard Deviation Channel 0: {class_name}')\n",
        "            axes[1].axis('off')\n",
        "            fig.colorbar(im1, ax=axes[1])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "             print(f\"No Channel 0 data for class '{class_name}'.\")\n",
        "\n",
        "\n",
        "        if stacked_images.shape[1] > 1:\n",
        "            average_channel_1 = torch.mean(stacked_images[:, 1, :, :], dim=0)\n",
        "            std_channel_1 = torch.std(stacked_images[:, 1, :, :], dim=0) # Calculate standard deviation\n",
        "\n",
        "            # Determine the min and max values across both average and standard deviation for Channel 1\n",
        "            vmin_ch1 = min(average_channel_1.min().item(), std_channel_1.min().item())\n",
        "            vmax_ch1 = max(average_channel_1.max().item(), std_channel_1.max().item())\n",
        "\n",
        "            # Plot average and standard deviation for Channel 1 side by side\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "            im0 = axes[0].imshow(average_channel_1.numpy(), cmap='viridis', vmin=vmin_ch1, vmax=vmax_ch1)\n",
        "            axes[0].set_title(f'Average Channel 1: {class_name}')\n",
        "            axes[0].axis('off')\n",
        "            fig.colorbar(im0, ax=axes[0])\n",
        "\n",
        "            im1 = axes[1].imshow(std_channel_1.numpy(), cmap='viridis', vmin=vmin_ch1, vmax=vmax_ch1)\n",
        "            axes[1].set_title(f'Standard Deviation Channel 1: {class_name}')\n",
        "            axes[1].axis('off')\n",
        "            fig.colorbar(im1, ax=axes[1])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"No Channel 1 data for class '{class_name}'.\")"
      ],
      "metadata": {
        "id": "3wWH7mE3GJaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "integri_label = dataset_to_use.classes.index('Integri')\n",
        "abbattuti_label = dataset_to_use.classes.index('Abbattuti')\n",
        "\n",
        "if integri_label in images_by_class and abbattuti_label in images_by_class:\n",
        "    # Get the average images for Integri\n",
        "    integri_images = images_by_class[integri_label]\n",
        "    if integri_images:\n",
        "        stacked_integri = torch.stack(integri_images)\n",
        "        avg_integri_ch0 = torch.mean(stacked_integri[:, 0, :, :], dim=0)\n",
        "        avg_integri_ch1 = torch.mean(stacked_integri[:, 1, :, :], dim=0)\n",
        "    else:\n",
        "        avg_integri_ch0 = None\n",
        "        avg_integri_ch1 = None\n",
        "        print(\"No images found for class 'Integri'. Cannot perform subtraction.\")\n",
        "\n",
        "\n",
        "    # Get the average images for Abbattuti\n",
        "    abbattuti_images = images_by_class[abbattuti_label]\n",
        "    if abbattuti_images:\n",
        "        stacked_abbattuti = torch.stack(abbattuti_images)\n",
        "        avg_abbattuti_ch0 = torch.mean(stacked_abbattuti[:, 0, :, :], dim=0)\n",
        "        avg_abbattuti_ch1 = torch.mean(stacked_abbattuti[:, 1, :, :], dim=0)\n",
        "    else:\n",
        "        avg_abbattuti_ch0 = None\n",
        "        avg_abbattuti_ch1 = None\n",
        "        print(\"No images found for class 'Abbattuti'. Cannot perform subtraction.\")\n",
        "\n",
        "\n",
        "    # Perform subtraction channel by channel and calculate magnitude if average images are available\n",
        "    if avg_integri_ch0 is not None and avg_abbattuti_ch0 is not None:\n",
        "        diff_channel_0 = avg_integri_ch0 - avg_abbattuti_ch0 # Real part difference\n",
        "        diff_channel_1 = avg_integri_ch1 - avg_abbattuti_ch1 # Imaginary part difference\n",
        "\n",
        "        print(\"Subtraction of average images performed channel by channel.\")\n",
        "\n",
        "        # Plot the difference images with color bars\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        im0 = plt.imshow(diff_channel_0.numpy(), cmap='seismic') # Use seismic colormap to show positive and negative differences\n",
        "        plt.title('Difference (Integri - Abbattuti) - Channel 0')\n",
        "        plt.axis('off')\n",
        "        plt.colorbar(im0) # Add color bar for Channel 0 difference\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        im1 = plt.imshow(diff_channel_1.numpy(), cmap='seismic')\n",
        "        plt.title('Difference (Integri - Abbattuti) - Channel 1')\n",
        "        plt.axis('off')\n",
        "        plt.colorbar(im1) # Add color bar for Channel 1 difference\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Average images for 'Integri' or 'Abbattuti' not found in images_by_class dictionary.\")\n",
        "        print(\"Please ensure the previous cell (cell_id: 3wWH7mE3GJaH) was run and successfully calculated the average images for these classes.\")"
      ],
      "metadata": {
        "id": "LnBIG-afM8Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa cella effettua il training del modello scelto su train_dataset"
      ],
      "metadata": {
        "id": "TS4Tqp55oOOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks7Zms55xWuz"
      },
      "outputs": [],
      "source": [
        "start_from_checkpoint = False\n",
        "save_dir = '/content/drive/MyDrive/Colab Notebooks/Towercheck/Model/'\n",
        "model_name = model_architecture+\"_cls_\"+str(num_classes)\n",
        "\n",
        "# Move the model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "model_trainer = ModelTrainer(model=model, device=device, loss_fun=nn.CrossEntropyLoss(),\n",
        "                             batch_size=batch_size, learning_rate=learning_rate,\n",
        "                             save_dir=save_dir, model_name=model_name,\n",
        "                             start_from_checkpoint=start_from_checkpoint)\n",
        "\n",
        "model_trainer.set_data(train_set=train_dataset, val_set=val_dataset, test_set=test_dataset)\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "valid_acc = 0\n",
        "train_acc = 0\n",
        "\n",
        "pbar = trange(start_epoch, num_epochs, leave=False, desc=\"Epoch\")\n",
        "for epoch in pbar:\n",
        "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Val %.2f%%' % (train_acc * 100, valid_acc * 100))\n",
        "\n",
        "    #Train su singole epoca\n",
        "    model_trainer.train_model()\n",
        "\n",
        "    #Valutazionedel modello su val\n",
        "    train_acc = model_trainer.evaluate_model(train_test_val=\"train\")\n",
        "    valid_acc = model_trainer.evaluate_model(train_test_val=\"val\")\n",
        "\n",
        "    #Salvataggio del miglior modello\n",
        "    if valid_acc > model_trainer.best_valid_acc:\n",
        "        model_trainer.save_checkpoint(epoch, valid_acc)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Training time %.2f seconds\" %(end_time - start_time))\n",
        "print(\"The highest validation accuracy was %.2f%%\" %(model_trainer.best_valid_acc*100))\n",
        "\n",
        "#Valutazione del modello sul set di test\n",
        "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
        "print(f\"\\nAccuracy on the test set: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plots\n",
        "_ = plt.figure(figsize = (10,5))\n",
        "train_x = np.linspace(0, num_epochs, len(model_trainer.train_loss_logger))\n",
        "_ = plt.plot(train_x, model_trainer.train_loss_logger, label=\"Training Loss\")\n",
        "\n",
        "if len(model_trainer.val_loss_logger) > 0:\n",
        "    valid_x = np.linspace(0, num_epochs, len(model_trainer.val_loss_logger))\n",
        "    _ = plt.plot(valid_x, model_trainer.val_loss_logger, label=\"Validation Loss\")\n",
        "\n",
        "_ = plt.title(\"Loss per Epoch\")\n",
        "_ = plt.xlabel(\"Epoch\")\n",
        "_ = plt.ylabel(\"Loss\")\n",
        "_ = plt.legend()\n",
        "plt.show()\n",
        "\n",
        "_ = plt.figure(figsize = (10,5))\n",
        "train_x = np.linspace(0, num_epochs, len(model_trainer.train_acc_logger))\n",
        "_ = plt.plot(train_x, model_trainer.train_acc_logger, c = \"y\")\n",
        "valid_x = np.linspace(0, num_epochs, len(model_trainer.val_acc_logger))\n",
        "_ = plt.plot(valid_x, model_trainer.val_acc_logger, c = \"k\")\n",
        "\n",
        "_ = plt.title(\"Accuracy\")\n",
        "_ = plt.xlabel(\"Epoch\")\n",
        "_ = plt.ylabel(\"Accuracy\")\n",
        "_ = plt.legend([\"Training accuracy\", \"Validation accuracy\"])\n"
      ],
      "metadata": {
        "id": "mhsE_8mdwxBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model_trainer.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "misclassified_images = []\n",
        "misclassified_labels = []\n",
        "misclassified_preds = []\n",
        "misclassified_filepaths = [] # List to store file paths of misclassified images\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Iterate through the test loader with index\n",
        "    for batch_idx, (inputs, labels) in enumerate(model_trainer.test_loader):\n",
        "        inputs = inputs.to(model_trainer.device)\n",
        "        labels = labels.to(model_trainer.device)\n",
        "        outputs = model_trainer(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Identify misclassified examples in the current batch\n",
        "        misclassified_mask = preds != labels\n",
        "        misclassified_indices_in_batch = torch.where(misclassified_mask)[0]\n",
        "\n",
        "        # Get the global indices of misclassified examples in the test dataset\n",
        "        # The index in the test_loader is batch_idx, the index within the batch is misclassified_indices_in_batch\n",
        "        # We need to map these back to the original dataset indices through the test_dataset Subset\n",
        "        for idx_in_batch in misclassified_indices_in_batch:\n",
        "            global_dataset_index = test_dataset.indices[batch_idx * model_trainer.batch_size + idx_in_batch]\n",
        "            misclassified_filepaths.append(dataset_to_use.image_paths[global_dataset_index])\n",
        "\n",
        "\n",
        "        misclassified_images.extend(inputs[misclassified_mask].cpu())\n",
        "        misclassified_labels.extend(labels[misclassified_mask].cpu().numpy())\n",
        "        misclassified_preds.extend(preds[misclassified_mask].cpu().numpy())\n",
        "\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Define class names based on the dataset used for validation\n",
        "class_names = dataset_to_use.classes # Access the classes directly from the dataset\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Test Set)')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print per-class accuracy, precision, and recall\n",
        "print(\"\\nPer-Class Metrics (Test Set):\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    true_positives = cm[i, i]\n",
        "    false_negatives = np.sum(cm[i, :]) - true_positives\n",
        "    false_positives = np.sum(cm[:, i]) - true_positives\n",
        "    true_negatives = np.sum(cm) - true_positives - false_positives - false_negatives\n",
        "\n",
        "    # Handle division by zero for precision and recall\n",
        "    accuracy = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"  {class_name}:\")\n",
        "    print(f\"    Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"    Precision: {precision:.2f}\")\n",
        "    print(f\"    Recall: {recall:.2f}\")\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"  {class_name}:\")\n",
        "    print(f\"    Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"    Precision: {precision:.2f}\")\n",
        "    print(f\"    Recall: {recall:.2f}\")\n",
        "    print(f\"    F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "# Print the file paths of misclassified examples\n",
        "print(\"\\nMisclassified Examples (File Paths):\")\n",
        "for filepath, true_lbl, pred_lbl in zip(misclassified_filepaths, misclassified_labels, misclassified_preds):\n",
        "    true_class_name = dataset_to_use.classes[true_lbl]\n",
        "    pred_class_name = dataset_to_use.classes[pred_lbl]\n",
        "    print(f\"  File: {filepath}, True Label: {true_class_name}, Predicted Label: {pred_class_name}\")"
      ],
      "metadata": {
        "id": "-vlTXnpWw4Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "# ---- Build model exactly as in training ----\n",
        "num_classes = 2\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "model.conv1 = torch.nn.Conv2d(\n",
        "    in_channels=2,\n",
        "    out_channels=model.conv1.out_channels,\n",
        "    kernel_size=model.conv1.kernel_size,\n",
        "    stride=model.conv1.stride,\n",
        "    padding=model.conv1.padding,\n",
        "    bias=model.conv1.bias is not None\n",
        ")\n",
        "model.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# ---- Load ONLY the model state dict from your checkpoint ----\n",
        "ckpt_path = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/Model/ResNet50_2classes_IA_extratgt.pt\"\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "state_dict = ckpt[\"model_state_dict\"]  # this exists in your file\n",
        "\n",
        "# (Optional) strip 'module.' if model was saved from DataParallel\n",
        "from collections import OrderedDict\n",
        "clean_sd = OrderedDict((k.replace(\"module.\", \"\", 1) if k.startswith(\"module.\") else k, v)\n",
        "                       for k, v in state_dict.items())\n",
        "\n",
        "model.load_state_dict(clean_sd, strict=True)  # this should succeed as-is ✅\n",
        "model.eval()\n",
        "\n",
        "# ---- Minimal Grad-CAM for ResNet-50 (target layer = layer4[-1]) ----\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.activations, self.gradients = None, None\n",
        "        self.h1 = target_layer.register_forward_hook(self._fwd_hook)\n",
        "        self.h2 = target_layer.register_full_backward_hook(self._bwd_hook)\n",
        "\n",
        "    def _fwd_hook(self, m, i, o): self.activations = o.detach()\n",
        "    def _bwd_hook(self, m, gi, go): self.gradients = go[0].detach()\n",
        "\n",
        "    def __call__(self, x, class_idx=None):\n",
        "        H, W = x.shape[-2:]\n",
        "        self.model.zero_grad()\n",
        "        with torch.enable_grad():\n",
        "            logits = self.model(x)\n",
        "            if class_idx is None:\n",
        "                class_idx = int(logits.argmax(dim=1))\n",
        "            score = logits[0, class_idx]\n",
        "        score.backward()\n",
        "        w = self.gradients.mean(dim=(2, 3), keepdim=True)     # (1,C,1,1)\n",
        "        cam = (w * self.activations).sum(dim=1, keepdim=True) # (1,1,h,w)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
        "        cam = (cam - cam.min()) / (cam.max() + 1e-8)\n",
        "        return cam  # (1,1,H,W)\n",
        "\n",
        "    def close(self):\n",
        "        self.h1.remove(); self.h2.remove()\n",
        "\n",
        "target_layer = model.layer4[-1]  # best default for ResNet-50\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "\n",
        "# ---- Example usage ----\n",
        "# x: your preprocessed 2-channel tensor (1,2,H,W) using the SAME normalization as training\n",
        "# x = ... .to(device)\n",
        "# cam = gradcam(x)           # (1,1,H,W) in [0,1]\n",
        "# heatmap = cam[0,0].cpu().numpy()\n",
        "# gradcam.close()\n"
      ],
      "metadata": {
        "id": "V_RZHm1qFyZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CONFIG ====\n",
        "DATASET_CHOICE = \"test\"  # one of: \"train\", \"val\", \"test\"\n",
        "NUM_SAMPLES = 20\n",
        "OUTPUT_DIR = \"cam_outputs\"\n",
        "SEED = 42\n",
        "\n",
        "# If you used per-channel normalization during training, put it here (length-2 lists).\n",
        "# If unknown, leave as None and the script will min-max normalize channel 0 for display only.\n",
        "TRAIN_MEAN = None  # e.g., [0.485, 0.456] for 2-ch\n",
        "TRAIN_STD  = None  # e.g., [0.229, 0.224]\n",
        "\n",
        "# ==== IMPORTS ====\n",
        "import os, random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "from collections import OrderedDict\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# ==== 1) PICK THE DATASET YOU WANT TO VISUALIZE ====\n",
        "# Assumes you already created these via random_split:\n",
        "#   train_dataset, val_dataset, test_dataset = random_split(dataset_to_use, [...])\n",
        "if DATASET_CHOICE == \"train\":\n",
        "    dataset_for_vis = train_dataset\n",
        "elif DATASET_CHOICE == \"val\":\n",
        "    dataset_for_vis = val_dataset\n",
        "else:\n",
        "    dataset_for_vis = test_dataset  # default\n",
        "\n",
        "# ==== 2) BUILD THE MODEL EXACTLY AS TRAINED ====\n",
        "num_classes = 2  # adjust if needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.conv1 = torch.nn.Conv2d(\n",
        "    in_channels=2,\n",
        "    out_channels=model.conv1.out_channels,\n",
        "    kernel_size=model.conv1.kernel_size,\n",
        "    stride=model.conv1.stride,\n",
        "    padding=model.conv1.padding,\n",
        "    bias=model.conv1.bias is not None,\n",
        ")\n",
        "model.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# ---- Load ONLY the model weights from your checkpoint ----\n",
        "# (Update the path below if you’re not on Colab/Drive)\n",
        "ckpt_path = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/Model/ResNet50_2classes_IA_extratgt.pt\"\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "state_dict = ckpt.get(\"model_state_dict\", ckpt)  # fall back if plain SD\n",
        "\n",
        "# strip \"module.\" if saved with DataParallel\n",
        "clean_sd = OrderedDict((k.replace(\"module.\", \"\", 1) if k.startswith(\"module.\") else k, v)\n",
        "                       for k, v in state_dict.items())\n",
        "model.load_state_dict(clean_sd, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# ==== 3) GRAD-CAM IMPLEMENTATION (last conv block) ====\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "        self.h1 = target_layer.register_forward_hook(self._fwd_hook)\n",
        "        self.h2 = target_layer.register_full_backward_hook(self._bwd_hook)\n",
        "\n",
        "    def _fwd_hook(self, module, inp, out):\n",
        "        self.activations = out.detach()           # (N, C, h, w)\n",
        "\n",
        "    def _bwd_hook(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()  # (N, C, h, w)\n",
        "\n",
        "    def __call__(self, x, class_idx=None):\n",
        "        # x: (1, 2, H, W) already preprocessed as in training\n",
        "        H, W = x.shape[-2:]\n",
        "        self.model.zero_grad()\n",
        "        with torch.enable_grad():\n",
        "            logits = self.model(x)\n",
        "            if class_idx is None:\n",
        "                class_idx = int(logits.argmax(dim=1).item())\n",
        "            score = logits[0, class_idx]\n",
        "        score.backward()\n",
        "\n",
        "        # weights: global-average gradients over spatial dims\n",
        "        w = self.gradients.mean(dim=(2, 3), keepdim=True)     # (1, C, 1, 1)\n",
        "        cam = (w * self.activations).sum(dim=1, keepdim=True) # (1, 1, h, w)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        # normalize to [0,1]\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / (cam.max() + 1e-8)\n",
        "        return cam  # (1,1,H,W)\n",
        "\n",
        "    def close(self):\n",
        "        self.h1.remove(); self.h2.remove()\n",
        "\n",
        "# Choose last conv block (best default for ResNet-50)\n",
        "target_layer = model.layer4[-1] if not hasattr(model, \"module\") else model.module.layer4[-1]\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "\n",
        "# ==== 4) SAMPLING 20 RANDOM SAMPLES ====\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "num_total = len(dataset_for_vis)\n",
        "indices = random.sample(range(num_total), k=min(NUM_SAMPLES, num_total))\n",
        "\n",
        "# Create output dir\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ==== 5) VISUALIZATION HELPERS ====\n",
        "import numpy as np\n",
        "\n",
        "def denorm_channel0_for_display(x_2ch):\n",
        "    \"\"\"\n",
        "    x_2ch: (1,2,H,W) tensor on any device (the SAME tensor fed to the model).\n",
        "    Returns np array (H,W) scaled to [0,1] for display of channel 0 only.\n",
        "    Uses TRAIN_MEAN/STD if provided, otherwise min-max.\n",
        "    \"\"\"\n",
        "    x0 = x_2ch[0, 0].detach().float().cpu()\n",
        "    if TRAIN_MEAN is not None and TRAIN_STD is not None:\n",
        "        # de-normalize channel 0 only\n",
        "        x0 = x0 * TRAIN_STD[0] + TRAIN_MEAN[0]\n",
        "        # simple [0,1] clamp for display if those stats are in that range\n",
        "        x0 = (x0 - x0.min()) / (x0.max() - x0.min() + 1e-8)\n",
        "    else:\n",
        "        # min-max normalize for display\n",
        "        x0 = (x0 - x0.min()) / (x0.max() - x0.min() + 1e-8)\n",
        "    return x0.cpu().numpy()\n",
        "\n",
        "def overlay_and_save(x_2ch, cam, out_path, title=None, alpha=0.4, show=False):\n",
        "    \"\"\"\n",
        "    x_2ch: (1,2,H,W) tensor\n",
        "    cam: (1,1,H,W) tensor in [0,1]\n",
        "    \"\"\"\n",
        "    bg = denorm_channel0_for_display(x_2ch)     # (H,W) in [0,1]\n",
        "    heat = cam[0,0].detach().cpu().numpy()      # (H,W) in [0,1]\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(bg, cmap=\"gray\")\n",
        "    plt.imshow(heat, cmap=\"jet\", alpha=alpha)\n",
        "    if title: plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    if show:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# ==== 6) RUN & PLOT (grid + individual files) ====\n",
        "ncols = 4\n",
        "nrows = int(np.ceil(len(indices) / ncols))\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
        "axes = np.atleast_2d(axes)\n",
        "\n",
        "# Get class names from the dataset\n",
        "class_names = dataset_for_vis.dataset.classes if hasattr(dataset_for_vis, 'dataset') and hasattr(dataset_for_vis.dataset, 'classes') else [str(i) for i in range(num_classes)]\n",
        "\n",
        "\n",
        "for idx_plot, ds_idx in enumerate(indices):\n",
        "    # dataset_for_vis[i] -> (image_tensor, label)\n",
        "    sample = dataset_for_vis[ds_idx]\n",
        "    if isinstance(sample, (tuple, list)):\n",
        "        img, label = sample[0], int(sample[1])\n",
        "    else:\n",
        "        img, label = sample, None\n",
        "\n",
        "    # Ensure shape (1,2,H,W)\n",
        "    if img.dim() == 3:\n",
        "        img_in = img.unsqueeze(0).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Expected image with shape (2,H,W); got {tuple(img.shape)}\")\n",
        "\n",
        "    # Get predicted label\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_in)\n",
        "        _, predicted_label = torch.max(outputs, 1)\n",
        "        predicted_label = predicted_label.item()\n",
        "\n",
        "\n",
        "    # Compute CAM for predicted class\n",
        "    with torch.enable_grad():\n",
        "        cam = gradcam(img_in)  # (1,1,H,W)\n",
        "\n",
        "    # Save individual file\n",
        "    out_file = os.path.join(OUTPUT_DIR, f\"cam_idx{ds_idx}.png\")\n",
        "    # Include true and predicted labels in the title\n",
        "    title = f\"idx {ds_idx} | True: {class_names[label]}\" + (f\" | Pred: {class_names[predicted_label]}\" if predicted_label is not None else \"\")\n",
        "    overlay_and_save(img_in, cam, out_file, title=title, alpha=0.40, show=False)\n",
        "\n",
        "    # Also draw in the grid\n",
        "    r, c = divmod(idx_plot, ncols)\n",
        "    ax = axes[r, c]\n",
        "    bg = denorm_channel0_for_display(img_in)\n",
        "    heat = cam[0,0].detach().cpu().numpy()\n",
        "    ax.imshow(bg, cmap=\"gray\")\n",
        "    ax.imshow(heat, cmap=\"jet\", alpha=0.40)\n",
        "    # Include true and predicted labels in the grid title\n",
        "    ax.set_title(f\"True: {class_names[label]}\\nPred: {class_names[predicted_label]}\", fontsize=10)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Hide any unused subplots\n",
        "for k in range(len(indices), nrows*ncols):\n",
        "    r, c = divmod(k, ncols)\n",
        "    axes[r, c].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cleanup hooks\n",
        "gradcam.close()\n",
        "print(f\"Saved individual overlays to: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "UBhUHwT70Q7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Assuming the necessary classes (ProcessedSARImageDataset, ModelTrainer) are defined in previous cells.\n",
        "\n",
        "# --- Configuration ---\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "save_dir = '/content/drive/MyDrive/Colab Notebooks/Towercheck/Model/'\n",
        "# Update model_name to match the model trained on the 2-class dataset with all subfolders\n",
        "model_name = 'ResNet50_2classes_IA' # Name of the model to load (from cell Ks7Zms55xWuz)\n",
        "\n",
        "\n",
        "# Subfolder to evaluate on\n",
        "evaluation_subfolder = ['_5x25_WithExtraTgt'] # Changed to the desired evaluation subfolder\n",
        "\n",
        "# Assuming the model was trained on 2 classes based on the model_name\n",
        "num_classes = 2\n",
        "selected_classes = ['Integri', 'Abbattuti'] # Define classes based on the trained model\n",
        "\n",
        "# Batch size used during training (or a suitable batch size for evaluation)\n",
        "batch_size = 64 # Adjust if necessary\n",
        "\n",
        "# Set device to GPU_indx if GPU is available\n",
        "GPU_indx = 0\n",
        "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Data Loading and Preparation for Evaluation ---\n",
        "\n",
        "# Load the evaluation dataset with the specified subfolder\n",
        "eval_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=selected_classes, transform=transforms.ToTensor(), selected_subfolders=evaluation_subfolder)\n",
        "\n",
        "# Assuming the model was trained with normalization, we need to apply the same normalization\n",
        "# We need the mean and std that were used during training. These were calculated in cell Ks7Zms55xWuz.\n",
        "# Based on the output of cell Ks7Zms55xWuz for 2 classes:\n",
        "mean = 0.042781053980191545\n",
        "std = 0.10918095029062695\n",
        "\n",
        "normalized_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,))\n",
        "])\n",
        "\n",
        "eval_dataset.transform = normalized_transform\n",
        "\n",
        "# Create a DataLoader for the evaluation dataset\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f'Number of evaluation examples: {len(eval_dataset)}')\n",
        "\n",
        "\n",
        "# --- Model Loading ---\n",
        "\n",
        "# Define the model architecture (ResNet50 modified for 2 input channels and num_classes output)\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.conv1 = nn.Conv2d(in_channels=2,\n",
        "                        out_channels=model.conv1.out_channels,\n",
        "                        kernel_size=model.conv1.kernel_size,\n",
        "                        stride=model.conv1.stride,\n",
        "                        padding=model.conv1.padding,\n",
        "                        bias=model.conv1.bias is not None\n",
        ")\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# Load the trained model state dictionary from the checkpoint\n",
        "model_path = os.path.join(save_dir, model_name + \".pt\")\n",
        "\n",
        "if os.path.isfile(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Model loaded successfully from {model_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model checkpoint not found at {model_path}\")\n",
        "\n",
        "# Move the model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "\n",
        "# Use the ModelTrainer class for evaluation\n",
        "# We need to instantiate ModelTrainer with the loaded model and evaluation loader\n",
        "# Although we are only evaluating, ModelTrainer requires some training parameters for initialization\n",
        "# We can use dummy values for learning_rate, save_dir, model_name as they are not used in evaluation mode\n",
        "model_trainer_eval = ModelTrainer(model=model, device=device, loss_fun=nn.CrossEntropyLoss(),\n",
        "                                  batch_size=batch_size, learning_rate=1e-5,\n",
        "                                  save_dir='./temp_eval', model_name='temp_eval_model',\n",
        "                                  start_from_checkpoint=False) # Set to False as we load the model manually\n",
        "\n",
        "# Set the evaluation loader in the model_trainer_eval instance\n",
        "model_trainer_eval.valid_loader = eval_loader # Use valid_loader for evaluation dataset\n",
        "\n",
        "\n",
        "# Evaluate the model on the evaluation dataset\n",
        "print(\"\\nEvaluating model on the specified subfolder...\")\n",
        "eval_acc = model_trainer_eval.evaluate_model(train_test_val=\"val\") # Use \"val\" as evaluate_model expects it\n",
        "\n",
        "print(f\"\\nAccuracy on {evaluation_subfolder[0]} dataset: {eval_acc*100:.2f}%\")\n",
        "\n",
        "# Optional: Generate confusion matrix for the evaluation dataset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions on the evaluation set\n",
        "model_trainer_eval.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in model_trainer_eval.valid_loader: # Use valid_loader\n",
        "        inputs = inputs.to(model_trainer_eval.device)\n",
        "        labels = labels.to(model_trainer_eval.device)\n",
        "        outputs = model_trainer_eval(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Define class names based on the selected_classes for evaluation\n",
        "class_names = selected_classes\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(f'Confusion Matrix ({evaluation_subfolder[0]} Dataset)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mQuG6RzjKkqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76f043d6"
      },
      "outputs": [],
      "source": [
        "# --- Logistic Regression (Option 2: two-logit softmax) ---\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import time\n",
        "from tqdm import trange\n",
        "\n",
        "# Paths & classes\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "two_classes   = ['Integri', 'Abbattuti']\n",
        "three_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "selected_subfolders = ['_0','_3x20_NoExtraTgt', '_5x25_NoExtraTgt']\n",
        "\n",
        "# --- 1) Build datasets WITHOUT normalization first (ToTensor only) ---\n",
        "base_transform = transforms.ToTensor()\n",
        "\n",
        "two_class_dataset = ProcessedSARImageDataset(\n",
        "    root_dir=data_dir,\n",
        "    selected_classes=two_classes,\n",
        "    transform=base_transform,\n",
        "    selected_subfolders=selected_subfolders\n",
        ")\n",
        "\n",
        "three_class_dataset = ProcessedSARImageDataset(\n",
        "    root_dir=data_dir,\n",
        "    selected_classes=three_classes,\n",
        "    transform=base_transform,\n",
        "    selected_subfolders=selected_subfolders\n",
        ")\n",
        "\n",
        "# Choose which dataset to use\n",
        "dataset_to_use = two_class_dataset  # or three_class_dataset\n",
        "num_classes = len(dataset_to_use.classes)\n",
        "\n",
        "\n",
        "# --- 2) Train/val split ---\n",
        "train_size = int(0.9 * len(dataset_to_use))\n",
        "val_size   = len(dataset_to_use) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset_to_use, [train_size, val_size])\n",
        "\n",
        "# --- 3) Compute per-channel mean/std on the TRAIN split only ---\n",
        "# IMPORTANT: do this while the underlying dataset transform is still ToTensor() only.\n",
        "batch_size = 8  # set your batch size (define if not already)\n",
        "def compute_mean_std(dataset):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        images = images.view(images.size(0), -1)  # Flatten\n",
        "        n += images.size(0)\n",
        "        mean += images.mean(1).sum().item()\n",
        "        std += images.std(1).sum().item()\n",
        "\n",
        "    mean /= n\n",
        "    std /= n\n",
        "    return mean, std\n",
        "\n",
        "    print(mean, std)\n",
        "\n",
        "mean, std = compute_mean_std(train_dataset)\n",
        "print(\"Per-channel mean:\", mean)\n",
        "print(\"Per-channel std :\", std)\n",
        "\n",
        "# --- 4) Set NORMALIZATION transform (no flatten here!) ---\n",
        "normalized_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                 # -> [2, 512, 512]\n",
        "    transforms.Normalize(mean=mean,std=std),\n",
        "])\n",
        "\n",
        "# Apply the new transform to the *underlying* dataset (affects both splits)\n",
        "train_dataset.dataset.transform = normalized_transform\n",
        "val_dataset.dataset.transform   = normalized_transform\n",
        "\n",
        "\n",
        "print(\"aa\", train_dataset[0][0].shape)\n",
        "for i in range(len(train_dataset)):\n",
        "\n",
        "    img, label = train_dataset[i]\n",
        "    if isinstance(img, torch.Tensor) and img.dim() == 3:\n",
        "        if img.shape[0] != 2:   # first dimension is channels\n",
        "            print(f\"Index {i}: shape {img.shape}, label {label}\")\n",
        "    else:\n",
        "        print(f\"Index {i}: unexpected type/shape -> {type(img)}, {getattr(img, 'shape', None)}\")\n",
        "\n",
        "\n",
        "# --- 5) Define the logistic regression model (flattens inside) ---\n",
        "class LogisticTwoLogits(nn.Module):\n",
        "    def __init__(self, C=2, H=512, W=512, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(C * H * W, num_classes)  # two logits for 2 classes\n",
        "\n",
        "    def forward(self, x):              # x: [N, 2, 512, 512]\n",
        "        x = self.flatten(x)            # -> [N, 524288]\n",
        "        return self.fc(x)              # -> [N, num_classes]\n",
        "\n",
        "\n",
        "# Infer C,H,W from a sample (after transform) just to be safe\n",
        "img0=train_dataset[0]\n",
        "C, H, W = img0[0].shape\n",
        "model = LogisticTwoLogits(C=C, H=H, W=W, num_classes=num_classes)\n",
        "\n",
        "# --- 6) Dataloaders ---\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
        "valid_loader = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Quick sanity check\n",
        "print(train_loader)\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(\"Batch images:\", xb.shape)     # [B, 2, 512, 512]\n",
        "print(\"Batch labels:\", yb.shape)     # [B]\n",
        "\n",
        "# --- 7) Training setup ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate = 1e-5       # set if not already\n",
        "start_epoch = 0                # set if not already\n",
        "num_epochs = 10                # set if not already\n",
        "\n",
        "# Use CrossEntropyLoss for 2-class softmax\n",
        "criterion = nn.CrossEntropyLoss()     # y must be LongTensor in {0,1}\n",
        "\n",
        "# If your ModelTrainer expects the loss in its ctor, pass criterion there.\n",
        "# Otherwise, we keep your original signature and ensure labels are long in the trainer.\n",
        "model_trainer = ModelTrainer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    loss_fun=criterion,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    save_dir='/content/drive/MyDrive/Colab Notebooks/Towercheck/Model/',\n",
        "    model_name=f'LogisticRegression_{num_classes}classes_v4',\n",
        "    start_from_checkpoint=False\n",
        ")\n",
        "\n",
        "# Provide data to trainer\n",
        "model_trainer.set_data(train_set=train_dataset, val_set=val_dataset)\n",
        "\n",
        "# --- 8) Train loop (unchanged logic) ---\n",
        "start_time = time.time()\n",
        "valid_acc = 0.0\n",
        "train_acc = 0.0\n",
        "\n",
        "pbar = trange(start_epoch, num_epochs, leave=False, desc=\"Epoch\")\n",
        "for epoch in pbar:\n",
        "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Val %.2f%%' % (train_acc * 100, valid_acc * 100))\n",
        "\n",
        "    # Train one epoch\n",
        "    model_trainer.train_model()   # Ensure inside this method: labels are cast to long: y = y.long()\n",
        "\n",
        "    # Evaluate\n",
        "    train_acc = model_trainer.evaluate_model(train_test_val=\"train\")\n",
        "    valid_acc = model_trainer.evaluate_model(train_test_val=\"val\")\n",
        "\n",
        "    # Save best\n",
        "    if valid_acc > model_trainer.best_valid_acc:\n",
        "        model_trainer.save_checkpoint(epoch, valid_acc)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Training time %.2f seconds\" %(end_time - start_time))\n",
        "print(\"The highest validation accuracy was %.2f%%\" %(model_trainer.best_valid_acc*100))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "_ = plt.figure(figsize = (10,5))\n",
        "train_x = np.linspace(0, num_epochs, len(model_trainer.train_loss_logger))\n",
        "_ = plt.plot(train_x, model_trainer.train_loss_logger, label=\"Training Loss\")\n",
        "\n",
        "if len(model_trainer.val_loss_logger) > 0: # Only plot validation loss if it's logged\n",
        "    valid_x = np.linspace(0, num_epochs, len(model_trainer.val_loss_logger))\n",
        "    _ = plt.plot(valid_x, model_trainer.val_loss_logger, label=\"Validation Loss\")\n",
        "\n",
        "_ = plt.title(\"Loss per Epoch\")\n",
        "_ = plt.xlabel(\"Epoch\")\n",
        "_ = plt.ylabel(\"Loss\")\n",
        "_ = plt.legend()\n",
        "plt.show() # Added to display the plot\n",
        "\n",
        "_ = plt.figure(figsize = (10,5))\n",
        "train_x = np.linspace(0, num_epochs, len(model_trainer.train_acc_logger))\n",
        "_ = plt.plot(train_x, model_trainer.train_acc_logger, c = \"y\")\n",
        "valid_x = np.linspace(0, num_epochs, len(model_trainer.val_acc_logger))\n",
        "_ = plt.plot(valid_x, model_trainer.val_acc_logger, c = \"k\")\n",
        "_ = plt.xlabel(\"Epoch\")\n",
        "_ = plt.ylabel(\"Accuracy\")\n",
        "_ = plt.title(\"Accuracy\")\n",
        "_ = plt.legend([\"Training accuracy\", \"Validation accuracy\"])\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions on the validation set\n",
        "model_trainer.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in model_trainer.valid_loader:\n",
        "        inputs = inputs.to(model_trainer.device)\n",
        "        labels = labels.to(model_trainer.device)\n",
        "        outputs = model_trainer(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Define class names based on the dataset used for validation\n",
        "class_names = dataset_to_use.classes # Access the classes directly from the dataset\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Validation Set)')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Assuming 'dataset' is already loaded and transformed with ToTensor()\n",
        "# If not, you would need to load it like in previous cells:\n",
        "# data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "# selected_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "# selected_subfolders = ['_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt']\n",
        "# dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=selected_classes, transform=transforms.ToTensor(), selected_subfolders=selected_subfolders)\n",
        "\n",
        "# Create lists to store the average intensity, average channel 0 values, and labels\n",
        "avg_intensity = []\n",
        "avg_channel_0 = []\n",
        "image_labels = []\n",
        "\n",
        "# Iterate through the dataset to calculate average intensity and channel 0 values\n",
        "for img, label in dataset:\n",
        "    # Ensure the image has at least two channels (real and imaginary)\n",
        "    if img.shape[0] >= 2:\n",
        "        # Assuming Channel 0 is real and Channel 1 is imaginary\n",
        "        real_part = img[0]\n",
        "        imag_part = img[1]\n",
        "\n",
        "        # Calculate intensity: sqrt(real^2 + imag^2)\n",
        "        intensity = torch.sqrt(real_part**2 + imag_part**2)\n",
        "\n",
        "        # Calculate the average intensity for the image\n",
        "        avg_intensity.append(torch.mean(intensity).item())\n",
        "\n",
        "        # Calculate the average of channel 0 (real part) for plotting against intensity\n",
        "        avg_channel_0.append(torch.mean(real_part).item())\n",
        "\n",
        "        image_labels.append(label)\n",
        "    else:\n",
        "        print(f\"Skipping image with unexpected shape: {img.shape}\")\n",
        "\n",
        "\n",
        "# Convert lists to NumPy arrays for plotting\n",
        "avg_intensity_np = np.array(avg_intensity)\n",
        "avg_channel_0_np = np.array(avg_channel_0)\n",
        "image_labels_np = np.array(image_labels)\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(avg_channel_0_np, avg_channel_1_np, c=image_labels_np, cmap='viridis', alpha=0.6)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Average Value of Channel 0 (Real Part)')\n",
        "plt.ylabel('Average Intensity (sqrt(Real^2 + Imag^2))')\n",
        "plt.title('Scatter Plot of Average Channel 0 vs. Average Intensity per Image')\n",
        "\n",
        "# Add a legend based on the class names from the dataset\n",
        "legend_elements = []\n",
        "for i, class_name in enumerate(dataset.classes):\n",
        "    # Get the color from the colormap corresponding to the class index\n",
        "    color = plt.cm.viridis(i / (len(dataset.classes) - 1))\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', label=class_name,\n",
        "                                      markerfacecolor=color, markersize=10))\n",
        "\n",
        "plt.legend(handles=legend_elements, title=\"Classes\")\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "icMA75dcuFLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b673c6c7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Assuming 'dataset' is already loaded and transformed with ToTensor()\n",
        "# If not, you would need to load it like in previous cells:\n",
        "# data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "# selected_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "# selected_subfolders = ['_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt']\n",
        "# dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=selected_classes, transform=transforms.ToTensor(), selected_subfolders=selected_subfolders)\n",
        "\n",
        "\n",
        "# Create a dictionary to store average intensities for each class\n",
        "average_intensities_by_class = {class_name: [] for class_name in dataset.classes}\n",
        "\n",
        "# Iterate through the dataset to calculate average intensity and group by class\n",
        "for img, label in dataset:\n",
        "    # Ensure the image has at least two channels (real and imaginary)\n",
        "    if img.shape[0] >= 2:\n",
        "        # Assuming Channel 0 is real and Channel 1 is imaginary\n",
        "        real_part = img[0]\n",
        "        imag_part = img[1]\n",
        "\n",
        "        # Calculate intensity: sqrt(real^2 + imag^2)\n",
        "        intensity = torch.sqrt(real_part**2 + imag_part**2)\n",
        "\n",
        "        # Calculate the average intensity for the image\n",
        "        avg_intensity = torch.mean(intensity).item()\n",
        "\n",
        "        # Get the class name for the current label\n",
        "        class_name = dataset.classes[label]\n",
        "\n",
        "        # Append the average intensity to the corresponding class list\n",
        "        average_intensities_by_class[class_name].append(avg_intensity)\n",
        "    else:\n",
        "        print(f\"Skipping image with unexpected shape: {img.shape}\")\n",
        "\n",
        "\n",
        "# Plot histograms for each class\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Determine the overall range of average intensities to set bins\n",
        "all_avg_intensities = [item for sublist in average_intensities_by_class.values() for item in sublist]\n",
        "\n",
        "print(num_bins)\n",
        "for class_name, avg_intensities in average_intensities_by_class.items():\n",
        "  min_intensity = min(avg_intensities)\n",
        "  max_intensity = max(avg_intensities)\n",
        "  bin_size = 0.005\n",
        "  num_bins = int((max_intensity - min_intensity) / bin_size)\n",
        "  plt.hist(avg_intensities, bins=num_bins, alpha=0.7, label=class_name)\n",
        "\n",
        "plt.xlabel('Average Intensity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Average Intensity per Class')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVboVs6oaLo2"
      },
      "outputs": [],
      "source": [
        "# === Visualize learned weights as images ===\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn # Import nn for defining the model structure\n",
        "import torch.nn.functional as F # Import functional for the linear layer\n",
        "\n",
        "# Define the structure of the Logistic Regression model used for training\n",
        "class LogisticTwoLogits(nn.Module):\n",
        "    def __init__(self, C=2, H=512, W=512, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(C * H * W, num_classes)  # two logits for 2 classes\n",
        "\n",
        "    def forward(self, x):              # x: [N, 2, 512, 512]\n",
        "        x = self.flatten(x)            # -> [N, 524288]\n",
        "        return self.fc(x)              # -> [N, num_classes]\n",
        "\n",
        "\n",
        "# Load the model state dictionary from the checkpoint file\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/Towercheck/Model/LogisticRegression_2classes_v2.pt' # Use the correct model name\n",
        "checkpoint = torch.load(model_path)\n",
        "\n",
        "# Instantiate the model with the correct dimensions and load the state dictionary\n",
        "# Infer C, H, W from the flattened dimension in the state_dict's fc.weight\n",
        "num_classes, input_dim = checkpoint['model_state_dict']['fc.weight'].shape\n",
        "C, H, W = 2, 512, 512 # Assuming C=2, H=512, W=512 based on data processing\n",
        "\n",
        "loaded_model = LogisticTwoLogits(C=C, H=H, W=W, num_classes=num_classes)\n",
        "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Grab weights from the linear layer of the loaded model\n",
        "weights = loaded_model.fc.weight.data.cpu().numpy()  # shape: [num_classes, input_dim]\n",
        "print(weights.shape)\n",
        "\n",
        "# Reshape each class's weights into (C, H, W)\n",
        "weights_img = weights.reshape(num_classes, C, H, W)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, num_classes, figsize=(5*num_classes, 5))\n",
        "\n",
        "# Define class names based on the dataset used for validation (assuming the same classes as in cell 76f043d6)\n",
        "class_names = ['Integri', 'Rimossi'] # Based on cell 76f043d6 which trained a 2-class model\n",
        "\n",
        "\n",
        "for i in range(num_classes):\n",
        "    ax = axes[i] if num_classes > 1 else axes\n",
        "\n",
        "    if C == 1:\n",
        "        # Single-channel input (grayscale)\n",
        "        ax.imshow(weights_img[i, 0], cmap=\"seismic\")\n",
        "    elif C == 2:\n",
        "        # Two-channel input -> show as two heatmaps side by side\n",
        "        # We can plot one of the channels, or both side by side\n",
        "        # Let's plot the second channel as in the original code cell KjCpzrIiPT-H\n",
        "        ax.imshow(weights_img[i, 1], cmap=\"seismic\", alpha=1)\n",
        "    else:\n",
        "        # If >2 channels, collapse to RGB with normalization\n",
        "        w = weights_img[i]\n",
        "        w = (w - w.min()) / (w.max() - w.min() + 1e-8)  # normalize 0-1\n",
        "        if w.shape[0] >= 3:\n",
        "            ax.imshow(np.transpose(w[:3], (1,2,0)))  # first 3 channels → RGB\n",
        "        else:\n",
        "            ax.imshow(w[0], cmap=\"seismic\")\n",
        "\n",
        "\n",
        "    ax.set_title(f\"Weights for class: {class_names[i]}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d98e24e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming weights_img is available from a previous cell (e.g., cell_id: VVboVs6oaLo2)\n",
        "# If not, you would need to load the model and extract weights as done in that cell.\n",
        "\n",
        "# Ensure weights_img has the expected shape (num_classes, C, H, W)\n",
        "# and that weights_img[1, 0] is a valid slice.\n",
        "if 'weights_img' in globals() and weights_img.shape[0] > 1 and weights_img.shape[1] > 0:\n",
        "    # Get the weights for the second class (index 1), first channel (index 0)\n",
        "    weights_channel_0_class_1 = weights_img[1, 0]\n",
        "\n",
        "    # Create a boolean mask for positive values\n",
        "    positive_mask = weights_channel_0_class_1 > 0\n",
        "\n",
        "    # Visualize the mask\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(positive_mask, cmap='gray') # Using a grayscale colormap for the mask\n",
        "    plt.title('Mask of Positive Weights (Class 1, Channel 0)')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Mask of positive values created and visualized.\")\n",
        "else:\n",
        "    print(\"Error: weights_img variable not found or does not have the expected shape.\")\n",
        "    print(\"Please run the cell that generates weights_img first (e.g., cell_id: VVboVs6oaLo2).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15e64c81"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming weights_img is available from a previous cell (e.g., cell_id: VVboVs6oaLo2)\n",
        "# If not, you would need to load the model and extract weights as done in that cell.\n",
        "\n",
        "# Ensure weights_img has the expected shape (num_classes, C, H, W)\n",
        "# and that the required slices are valid.\n",
        "if 'weights_img' in globals() and weights_img.shape[0] >= 2 and weights_img.shape[1] > 0:\n",
        "    # Get the weights for the first class (index 0), first channel (index 0)\n",
        "    weights_channel_0_class_0 = weights_img[0, 0]\n",
        "\n",
        "    # Get the weights for the second class (index 1), first channel (index 0)\n",
        "    weights_channel_0_class_1 = weights_img[1, 0]\n",
        "\n",
        "    # Create boolean masks for positive values\n",
        "    positive_mask_class_0 = weights_channel_0_class_0 > 0\n",
        "    positive_mask_class_1 = weights_channel_0_class_1 > 0\n",
        "\n",
        "    # Visualize the masks side-by-side\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    axes[0].imshow(positive_mask_class_0, cmap='gray')\n",
        "    axes[0].set_title('Mask of Positive Weights (Class 0, Channel 0)')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(positive_mask_class_1, cmap='gray')\n",
        "    axes[1].set_title('Mask of Positive Weights (Class 1, Channel 0)')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Masks of positive values for Channel 0 of Class 0 and Class 1 created and visualized side-by-side.\")\n",
        "else:\n",
        "    print(\"Error: weights_img variable not found or does not have the expected shape.\")\n",
        "    print(\"Please run the cell that generates weights_img first (e.g., cell_id: VVboVs6oaLo2).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlRWHqw4r676"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "# Assuming ProcessedSARImageDataset class is defined in a previous cell\n",
        "\n",
        "# Define the data directory\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "\n",
        "# Define the class lists to include\n",
        "selected_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "\n",
        "# Define the subfolders to include within each class\n",
        "selected_subfolders = ['_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt']\n",
        "\n",
        "# Instantiate the dataset with ToTensor transform to get tensors\n",
        "dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=selected_classes, transform=transforms.ToTensor(), selected_subfolders=selected_subfolders)\n",
        "\n",
        "# Iterate through each class\n",
        "for class_name in dataset.classes:\n",
        "    class_label = dataset.classes.index(class_name)\n",
        "\n",
        "    # Filter images belonging to the current class\n",
        "    class_images = [img for img, label in dataset if label == class_label]\n",
        "\n",
        "    if not class_images:\n",
        "        print(f\"No images found for class '{class_name}'.\")\n",
        "        continue\n",
        "\n",
        "    # Separate channels\n",
        "    channel_0_images = [img[0] for img in class_images if img.shape[0] > 0]\n",
        "    channel_1_images = [img[1] for img in class_images if img.shape[0] > 1]\n",
        "\n",
        "    # Calculate and plot the average image for Channel 0\n",
        "    if channel_0_images:\n",
        "        channel_0_stacked = torch.stack(channel_0_images)\n",
        "        average_channel_0 = torch.mean(channel_0_stacked, dim=0)\n",
        "\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(average_channel_0.numpy(), cmap='viridis')\n",
        "        plt.title(f'Average Channel 0 Image for Class: {class_name}')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No images with Channel 0 found for class '{class_name}'.\")\n",
        "\n",
        "    # Calculate and plot the average image for Channel 1\n",
        "    if channel_1_images:\n",
        "        channel_1_stacked = torch.stack(channel_1_images)\n",
        "        average_channel_1 = torch.mean(channel_1_stacked, dim=0)\n",
        "\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(average_channel_1.numpy(), cmap='viridis')\n",
        "        plt.title(f'Average Channel 1 Image for Class: {class_name}')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No images with Channel 1 found for class '{class_name}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f43fe18"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Assuming 'dataset' is loaded and has the ToTensor() transform applied\n",
        "# Assuming 'positive_mask_class_1' is available from a previous cell (e.g., cell_id: 15e64c81)\n",
        "\n",
        "if 'positive_mask_class_1' in globals() and 'dataset' in globals():\n",
        "    # Ensure the mask is a PyTorch tensor for easier element-wise operations\n",
        "    if not isinstance(positive_mask_class_1, torch.Tensor):\n",
        "        positive_mask_class_1_tensor = torch.from_numpy(positive_mask_class_1).bool()\n",
        "    else:\n",
        "        positive_mask_class_1_tensor = positive_mask_class_1.bool()\n",
        "\n",
        "    # Create a list to store the average masked values for each image\n",
        "    average_masked_values_class_1 = []\n",
        "    image_labels = []\n",
        "\n",
        "    # Iterate through the dataset\n",
        "    for img, label in dataset:\n",
        "        # Ensure the image has at least the channel we are masking (channel 0)\n",
        "        if img.shape[0] > 0:\n",
        "            # Get the first channel of the image\n",
        "            channel_0_image = img[0] # Assuming Channel 0\n",
        "\n",
        "            # Apply the mask to the first channel\n",
        "            masked_image_channel_0 = channel_0_image * positive_mask_class_1_tensor\n",
        "\n",
        "            # Calculate the average of the masked values\n",
        "            # To avoid division by zero if the mask is all False, we can check if there are any positive mask values\n",
        "            if torch.sum(positive_mask_class_1_tensor) > 0:\n",
        "                average_masked_value = torch.sum(masked_image_channel_0) / torch.sum(positive_mask_class_1_tensor)\n",
        "            else:\n",
        "                average_masked_value = 0.0 # Or np.nan, depending on how you want to handle this case\n",
        "\n",
        "            average_masked_values_class_1.append(average_masked_value.item())\n",
        "            image_labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipping image with unexpected shape: {img.shape}\")\n",
        "\n",
        "    # Convert the list of average masked values and labels to NumPy arrays\n",
        "    average_masked_values_class_1_np = np.array(average_masked_values_class_1)\n",
        "    image_labels_np = np.array(image_labels)\n",
        "\n",
        "    print(\"Average masked values for Class 1 calculated for each example.\")\n",
        "    print(f\"Shape of average masked values array: {average_masked_values_class_1_np.shape}\")\n",
        "    print(f\"Shape of image labels array: {image_labels_np.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'positive_mask_class_1' or 'dataset' not found.\")\n",
        "    print(\"Please ensure 'positive_mask_class_1' is generated first (e.g., by running cell_id: 15e64c81) and 'dataset' is loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8f7ac19"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'average_masked_values_class_1_np' and 'image_labels_np' are available\n",
        "# from a previous cell (e.g., cell_id: 8f43fe18) and 'dataset' is loaded\n",
        "\n",
        "if 'average_masked_values_class_1_np' in globals() and 'image_labels_np' in globals() and 'dataset' in globals():\n",
        "    # Create a dictionary to store average masked values for each class\n",
        "    average_masked_values_by_class = {class_name: [] for class_name in dataset.classes}\n",
        "\n",
        "    # Populate the dictionary with the calculated average masked values, grouped by class\n",
        "    for i in range(len(image_labels_np)):\n",
        "        label = image_labels_np[i]\n",
        "        class_name = dataset.classes[label]\n",
        "        average_masked_values_by_class[class_name].append(average_masked_values_class_1_np[i])\n",
        "\n",
        "    # Plot histograms for each class\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for class_name, avg_masked_values in average_masked_values_by_class.items():\n",
        "        plt.hist(avg_masked_values, bins=50, alpha=0.7, label=class_name) # Adjust bins as needed\n",
        "\n",
        "    plt.xlabel('Average Masked Value (Class 1, Channel 0 Mask)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Average Masked Values per Class')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Error: Required variables ('average_masked_values_class_1_np', 'image_labels_np', or 'dataset') not found.\")\n",
        "    print(\"Please ensure the cell calculating average masked values (e.g., cell_id: 8f43fe18) and the cell loading the dataset have been run.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02dfa0eb"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt # Import matplotlib.pyplot\n",
        "\n",
        "# Assuming 'dataset' is loaded and has the ToTensor() transform applied\n",
        "# Assuming 'positive_mask_class_0' is available from a previous cell (e.g., cell_id: 15e64c81)\n",
        "\n",
        "if 'positive_mask_class_0' in globals() and 'dataset' in globals():\n",
        "    # Ensure the mask is a PyTorch tensor for easier element-wise operations\n",
        "    if not isinstance(positive_mask_class_0, torch.Tensor):\n",
        "        positive_mask_class_0_tensor = torch.from_numpy(positive_mask_class_0).bool()\n",
        "    else:\n",
        "        positive_mask_class_0_tensor = positive_mask_class_0.bool()\n",
        "\n",
        "    # Create a list to store the average masked values for each image\n",
        "    average_masked_values_class_0 = []\n",
        "    image_labels = []\n",
        "\n",
        "    # Iterate through the dataset\n",
        "    for img, label in dataset:\n",
        "        # Ensure the image has at least the channel we are masking (channel 0)\n",
        "        if img.shape[0] > 0:\n",
        "            # Get the first channel of the image\n",
        "            channel_0_image = img[0] # Assuming Channel 0\n",
        "\n",
        "            # Apply the mask to the first channel\n",
        "            masked_image_channel_0 = channel_0_image * positive_mask_class_0_tensor\n",
        "\n",
        "            # Calculate the average of the masked values\n",
        "            # To avoid division by zero if the mask is all False, we can check if there are any positive mask values\n",
        "            if torch.sum(positive_mask_class_0_tensor) > 0:\n",
        "                average_masked_value = torch.sum(masked_image_channel_0) / torch.sum(positive_mask_class_0_tensor)\n",
        "            else:\n",
        "                average_masked_value = 0.0 # Or np.nan, depending on how you want to handle this case\n",
        "\n",
        "            average_masked_values_class_0.append(average_masked_value.item())\n",
        "            image_labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipping image with unexpected shape: {img.shape}\")\n",
        "\n",
        "    # Convert the list of average masked values and labels to NumPy arrays\n",
        "    average_masked_values_class_0_np = np.array(average_masked_values_class_0)\n",
        "    image_labels_np = np.array(image_labels)\n",
        "\n",
        "    print(\"Average masked values for Class 0 calculated for each example.\")\n",
        "    print(f\"Shape of average masked values array: {average_masked_values_class_0_np.shape}\")\n",
        "    print(f\"Shape of image labels array: {image_labels_np.shape}\")\n",
        "\n",
        "    # --- Plot histogram divided by class ---\n",
        "    # Create a dictionary to store average masked values for each class for plotting\n",
        "    average_masked_values_by_class_plot = {class_name: [] for class_name in dataset.classes}\n",
        "\n",
        "    # Populate the dictionary with the calculated average masked values, grouped by class\n",
        "    for i in range(len(image_labels_np)):\n",
        "        label = image_labels_np[i]\n",
        "        class_name = dataset.classes[label]\n",
        "        average_masked_values_by_class_plot[class_name].append(average_masked_values_class_0_np[i])\n",
        "\n",
        "    # Determine the overall range of average masked values to set bins\n",
        "    all_avg_masked_values = [item for sublist in average_masked_values_by_class_plot.values() for item in sublist]\n",
        "    min_value = min(all_avg_masked_values)\n",
        "    max_value = max(all_avg_masked_values)\n",
        "    bin_width = 0.005 # Desired bin width\n",
        "    bins = np.arange(min_value, max_value + bin_width, bin_width) # Create bins with fixed width\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for class_name, avg_masked_values in average_masked_values_by_class_plot.items():\n",
        "        plt.hist(avg_masked_values, bins=bins, alpha=0.7, label=class_name) # Use the defined bins\n",
        "\n",
        "    plt.xlabel('Average Masked Value (Class 0, Channel 0 Mask)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Average Masked Values per Class (Class 0 Mask)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'positive_mask_class_0' or 'dataset' not found.\")\n",
        "    print(\"Please ensure 'positive_mask_class_0' is generated first (e.g., by running cell_id: 15e64c81) and 'dataset' is loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGMIJgXnoH-7"
      },
      "outputs": [],
      "source": [
        "# Load the model state dictionary\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/Towercheck/Model/LogisticRegression_2classes.pt'\n",
        "checkpoint = torch.load(model_path)\n",
        "\n",
        "# Extract the model state dictionary\n",
        "# The state_dict directly contains 'weight' and 'bias' keys, not 'linear.weight' and 'linear.bias'\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "# Create a dummy model to load the weights into (assuming the structure is the same as LogisticRegressionTorch)\n",
        "# We need the input_dim and num_classes from the training setup\n",
        "# Based on previous cells, the saved weights have input_dim 786432\n",
        "input_dim = 786432 # Input dimension from the saved checkpoint\n",
        "num_classes = 2 # Number of classes in the saved model (based on filename)\n",
        "\n",
        "class LogisticRegressionTorch(nn.Module):\n",
        "    def __init__(self, in_dim, n_classes):\n",
        "        super().__init__()\n",
        "        # The keys in the saved state_dict are 'weight' and 'bias'\n",
        "        self.weight = nn.Parameter(torch.empty(n_classes, in_dim))\n",
        "        self.bias = nn.Parameter(torch.empty(n_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x.view(x.size(0), -1), self.weight, self.bias)\n",
        "\n",
        "# We also need to import the functional module\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "loaded_model = LogisticRegressionTorch(input_dim, num_classes)\n",
        "loaded_model.load_state_dict(model_state_dict)\n",
        "print (loaded_model)\n",
        "# Grab weights from the linear layer of the loaded model\n",
        "weights = loaded_model.weight.data.cpu().numpy()  # shape: [num_classes, input_dim]\n",
        "print(weights.shape)\n",
        "\n",
        "# Reshape each class's weights into (C, H, W)\n",
        "# C, H, W are the channel, height, and width of the original images used for training\n",
        "# Assuming C=2, H=512, W=512 based on previous cells and the total input_dim\n",
        "C, H, W = 3, 512, 512\n",
        "weights_img = weights.reshape(num_classes, C, H, W)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, num_classes, figsize=(5*num_classes, 5))\n",
        "\n",
        "# Define class names based on the saved model\n",
        "class_names = ['Integri', 'Rimossi'] # Based on cell 'a5e99ba0'\n",
        "\n",
        "for i in range(num_classes):\n",
        "    ax = axes[i] if num_classes > 1 else axes\n",
        "\n",
        "    if C == 1:\n",
        "        # Single-channel input (grayscale)\n",
        "        ax.imshow(weights_img[i, 0], cmap=\"seismic\")\n",
        "    elif C == 2:\n",
        "        # Two-channel input -> show as two heatmaps side by side\n",
        "        # We can plot one of the channels, or both side by side\n",
        "        # Let's plot the second channel as in the original code cell KjCpzrIiPT-H\n",
        "        ax.imshow(weights_img[i, 1], cmap=\"seismic\", alpha=1)\n",
        "    elif C == 3:\n",
        "        # Three-channel input -> plot the first 2 channels\n",
        "        # We can plot them side by side or overlay them\n",
        "        # For simplicity, let's plot the first channel as a heatmap\n",
        "        # You could also plot weights_img[i, 1] for the second channel\n",
        "        ax.imshow(weights_img[i, 0], cmap=\"seismic\")\n",
        "        ax.set_title(f\"Weights for class: {class_names[i]} (Channel 1)\")\n",
        "    else:\n",
        "        # If >3 channels or unexpected, collapse to grayscale or show first channel\n",
        "        w = weights_img[i]\n",
        "        # For visualization, take the mean across channels or select one channel\n",
        "        w_gray = np.mean(w, axis=0) # Take mean across channels\n",
        "\n",
        "        ax.imshow(w_gray, cmap='viridis') # Use a suitable colormap\n",
        "        ax.set_title(f\"Weights for class: {class_names[i]} (Mean Channels)\")\n",
        "\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg3gmn-HUB7d"
      },
      "outputs": [],
      "source": [
        "print(img.shape)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(img[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c4263fb"
      },
      "outputs": [],
      "source": [
        "# Plot weight maps for each class\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os# === Flexible Logistic Regression (PyTorch, 2 or 3 classes) ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#Load the dataset\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "\n",
        "# Define the class lists\n",
        "two_classes = ['Integri', 'Rimossi']\n",
        "three_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "selected_subfolders = ['_0']#, '_3x20_NoExtraTgt', '_5x25_NoExtraTgt'] # Updated subfolder names\n",
        "\n",
        "\n",
        "# Create the datasets first, before normalization\n",
        "two_class_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=two_classes, transform=transforms.ToTensor(),  selected_subfolders=selected_subfolders)\n",
        "three_class_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=three_classes, transform=transforms.ToTensor(),  selected_subfolders=selected_subfolders)\n",
        "\n",
        "\n",
        "# Choose the dataset (either two_class_dataset or three_class_dataset)\n",
        "dataset_to_use = two_class_dataset  # Or three_class_dataset\n",
        "\n",
        "# Define the model based on the number of classes in the chosen dataset\n",
        "num_classes = len(dataset_to_use.classes)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_size = int(0.9 * len(dataset_to_use))\n",
        "val_size = len(dataset_to_use) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset_to_use, [train_size, val_size])\n",
        "\n",
        "def compute_mean_std(dataset):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        images = images.view(images.size(0), -1)  # Flatten\n",
        "        n += images.size(0)\n",
        "        mean += images.mean(1).sum().item()\n",
        "        std += images.std(1).sum().item()\n",
        "\n",
        "    mean /= n\n",
        "    std /= n\n",
        "    return mean, std\n",
        "\n",
        "mean, std = compute_mean_std(train_dataset)\n",
        "print(\"mean: \", mean)\n",
        "print(\"std: \", std)\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "#Redefine transforms with normalization\n",
        "normalized_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,))\n",
        "])\n",
        "\n",
        "#Reload datasets with normalization\n",
        "train_dataset.dataset.transform = normalized_transform\n",
        "val_dataset.dataset.transform = normalized_transform\n",
        "\n",
        "\n",
        "# Infer input shape\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(train_loader))[0]\n",
        "C, H, W = sample_batch.shape[1], sample_batch.shape[2], sample_batch.shape[3]\n",
        "input_dim = C * H * W\n",
        "\n",
        "num_classes = len(dataset_to_use.classes)\n",
        "\n",
        "class LogisticRegressionTorch(nn.Module):\n",
        "    def __init__(self, in_dim, n_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_dim, n_classes)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x.view(x.size(0), -1))\n",
        "\n",
        "logreg_model = LogisticRegressionTorch(input_dim, num_classes).to(device)\n",
        "\n",
        "# Optional class weights\n",
        "def compute_class_weights(loader, n_classes):\n",
        "    counts = torch.zeros(n_classes, dtype=torch.long)\n",
        "    for _, y in loader:\n",
        "        counts += torch.bincount(y, minlength=n_classes)\n",
        "    w = 1.0 / torch.clamp(counts.float(), min=1.0)\n",
        "    return (w / w.sum() * n_classes).to(device)\n",
        "\n",
        "use_class_weights = True\n",
        "weights = compute_class_weights(train_loader, num_classes) if use_class_weights else None\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = optim.SGD(logreg_model.parameters(), lr=1e-6, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "num_epochs = 30\n",
        "best_val_acc, best_state = 0.0, None\n",
        "\n",
        "# Logs\n",
        "train_losses, train_accuracies, val_accuracies = [], [], []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training ---\n",
        "    logreg_model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = logreg_model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "    train_acc = correct / max(total, 1)\n",
        "\n",
        "    # --- Validation ---\n",
        "    logreg_model.eval()\n",
        "    v_correct, v_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in valid_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = logreg_model(xb).argmax(1)\n",
        "            v_correct += (preds == yb).sum().item()\n",
        "            v_total += yb.size(0)\n",
        "\n",
        "    val_acc = v_correct / max(v_total, 1)\n",
        "\n",
        "    # Store logs\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc, best_state = val_acc, {k: v.cpu().clone() for k, v in logreg_model.state_dict().items()}\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{num_epochs} | \"\n",
        "          f\"Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "# Restore best\n",
        "if best_state is not None:\n",
        "    logreg_model.load_state_dict(best_state)\n",
        "\n",
        "# --- Plot curves ---\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\"); plt.legend(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(range(1, num_epochs+1), [a*100 for a in train_accuracies], label=\"Train Acc\")\n",
        "plt.plot(range(1, num_epochs+1), [a*100 for a in val_accuracies], label=\"Val Acc\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Accuracy per Epoch\"); plt.legend(); plt.show()\n",
        "\n",
        "# --- Final eval ---\n",
        "all_preds, all_labels = [], []\n",
        "logreg_model.eval()\n",
        "with torch.no_grad():\n",
        "    for xb, yb in valid_loader:\n",
        "        preds = logreg_model(xb.to(device)).argmax(1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(yb.numpy())\n",
        "\n",
        "print(f\"\\nBest Validation Accuracy: {best_val_acc*100:.2f}%\")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=dataset_to_use.classes,\n",
        "            yticklabels=dataset_to_use.classes)\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix (Validation)\"); plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=dataset_to_use.classes, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjCpzrIiPT-H"
      },
      "outputs": [],
      "source": [
        "# === Visualize learned weights as images ===\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Grab weights from the linear layer\n",
        "weights = logreg_model.linear.weight.data.cpu().numpy()  # shape: [num_classes, input_dim]\n",
        "print(weights.shape)\n",
        "# Reshape each class's weights into (C, H, W)\n",
        "weights_img = weights.reshape(num_classes, C, H, W)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, num_classes, figsize=(5*num_classes, 5))\n",
        "\n",
        "for i in range(num_classes):\n",
        "    ax = axes[i] if num_classes > 1 else axes\n",
        "\n",
        "    if C == 1:\n",
        "        # Single-channel input (grayscale)\n",
        "        ax.imshow(weights_img[i, 0], cmap=\"seismic\")\n",
        "    elif C == 2:\n",
        "        # Two-channel input → show as two heatmaps side by side\n",
        "        #ax.imshow(weights_img[i, 0], cmap=\"seismic\", alpha=0.7)\n",
        "        ax.imshow(weights_img[i, 1], cmap=\"seismic\", alpha=1)\n",
        "    else:\n",
        "        # If >2 channels, collapse to RGB with normalization\n",
        "        w = weights_img[i]\n",
        "        w = (w - w.min()) / (w.max() - w.min() + 1e-8)  # normalize 0-1\n",
        "        if w.shape[0] >= 3:\n",
        "            ax.imshow(np.transpose(w[:3], (1,2,0)))  # first 3 channels → RGB\n",
        "        else:\n",
        "            ax.imshow(w[0], cmap=\"seismic\")\n",
        "\n",
        "    ax.set_title(f\"Weights for class: {dataset_to_use.classes[i]}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIyAeWlezh-C"
      },
      "outputs": [],
      "source": [
        "# === Flexible Logistic Regression (PyTorch, 2 or 3 classes) ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "#Load the dataset\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "\n",
        "# Define the class lists\n",
        "two_classes = ['Integri', 'Rimossi']\n",
        "three_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "selected_subfolders = ['_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt'] # Updated subfolder names\n",
        "\n",
        "\n",
        "# Create the datasets first, before normalization\n",
        "two_class_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=two_classes, transform=transforms.ToTensor(),  selected_subfolders=selected_subfolders)\n",
        "three_class_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=three_classes, transform=transforms.ToTensor(),  selected_subfolders=selected_subfolders)\n",
        "\n",
        "\n",
        "# Choose the dataset (either two_class_dataset or three_class_dataset)\n",
        "dataset_to_use = two_class_dataset  # Or three_class_dataset\n",
        "\n",
        "# Define the model based on the number of classes in the chosen dataset\n",
        "num_classes = len(dataset_to_use.classes)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_size = int(0.9 * len(dataset_to_use))\n",
        "val_size = len(dataset_to_use) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset_to_use, [train_size, val_size])\n",
        "\n",
        "def compute_mean_std(dataset):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        images = images.view(images.size(0), -1)  # Flatten\n",
        "        n += images.size(0)\n",
        "        mean += images.mean(1).sum().item()\n",
        "        std += images.std(1).sum().item()\n",
        "\n",
        "    mean /= n\n",
        "    std /= n\n",
        "    return mean, std\n",
        "\n",
        "mean, std = compute_mean_std(train_dataset)\n",
        "print(\"mean: \", mean)\n",
        "print(\"std: \", std)\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "#Redefine transforms with normalization\n",
        "normalized_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,))\n",
        "])\n",
        "\n",
        "#Reload datasets with normalization\n",
        "train_dataset.dataset.transform = normalized_transform\n",
        "val_dataset.dataset.transform = normalized_transform\n",
        "\n",
        "\n",
        "# Infer input shape\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(train_loader))[0]\n",
        "C, H, W = sample_batch.shape[1], sample_batch.shape[2], sample_batch.shape[3]\n",
        "input_dim = C * H * W\n",
        "\n",
        "num_classes = len(dataset_to_use.classes)\n",
        "\n",
        "class LogisticRegressionTorch(nn.Module):\n",
        "    def __init__(self, in_dim, n_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_dim, n_classes)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x.view(x.size(0), -1))\n",
        "\n",
        "logreg_model = LogisticRegressionTorch(input_dim, num_classes).to(device)\n",
        "\n",
        "# Optional class weights\n",
        "def compute_class_weights(loader, n_classes):\n",
        "    counts = torch.zeros(n_classes, dtype=torch.long)\n",
        "    for _, y in loader:\n",
        "        counts += torch.bincount(y, minlength=n_classes)\n",
        "    w = 1.0 / torch.clamp(counts.float(), min=1.0)\n",
        "    return (w / w.sum() * n_classes).to(device)\n",
        "\n",
        "use_class_weights = True\n",
        "weights = compute_class_weights(train_loader, num_classes) if use_class_weights else None\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = optim.SGD(logreg_model.parameters(), lr=1e-6, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "num_epochs = 30\n",
        "best_val_acc, best_state = 0.0, None\n",
        "\n",
        "# Logs\n",
        "train_losses, train_accuracies, val_accuracies = [], [], []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training ---\n",
        "    logreg_model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = logreg_model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "    train_acc = correct / max(total, 1)\n",
        "\n",
        "    # --- Validation ---\n",
        "    logreg_model.eval()\n",
        "    v_correct, v_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in valid_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = logreg_model(xb).argmax(1)\n",
        "            v_correct += (preds == yb).sum().item()\n",
        "            v_total += yb.size(0)\n",
        "\n",
        "    val_acc = v_correct / max(v_total, 1)\n",
        "\n",
        "    # Store logs\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc, best_state = val_acc, {k: v.cpu().clone() for k, v in logreg_model.state_dict().items()}\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{num_epochs} | \"\n",
        "          f\"Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "# Restore best\n",
        "if best_state is not None:\n",
        "    logreg_model.load_state_dict(best_state)\n",
        "\n",
        "# --- Plot curves ---\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\"); plt.legend(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(range(1, num_epochs+1), [a*100 for a in train_accuracies], label=\"Train Acc\")\n",
        "plt.plot(range(1, num_epochs+1), [a*100 for a in val_accuracies], label=\"Val Acc\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Accuracy per Epoch\"); plt.legend(); plt.show()\n",
        "\n",
        "# --- Final eval ---\n",
        "all_preds, all_labels = [], []\n",
        "logreg_model.eval()\n",
        "with torch.no_grad():\n",
        "    for xb, yb in valid_loader:\n",
        "        preds = logreg_model(xb.to(device)).argmax(1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(yb.numpy())\n",
        "\n",
        "print(f\"\\nBest Validation Accuracy: {best_val_acc*100:.2f}%\")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=dataset_to_use.classes,\n",
        "            yticklabels=dataset_to_use.classes)\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix (Validation)\"); plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=dataset_to_use.classes, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddce08cf"
      },
      "source": [
        "# Task\n",
        "Run a PCA on the images loaded using the `ProcessedSARImageDataset` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc128917"
      },
      "source": [
        "## Load the image data\n",
        "\n",
        "### Subtask:\n",
        "Load the images from the dataset using the `ProcessedSARImageDataset` class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70f82089"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the images using the `ProcessedSARImageDataset` class as per the instructions. This involves defining the data directory, selected classes, and selected subfolders, and then instantiating the dataset object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbbc8904"
      },
      "outputs": [],
      "source": [
        "# Define the data directory\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "\n",
        "# Define the class lists to include\n",
        "selected_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "\n",
        "# Define the subfolders to include within each class\n",
        "selected_subfolders = ['_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt']\n",
        "\n",
        "# Create the dataset without any transforms yet\n",
        "dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=selected_classes, transform=None, selected_subfolders=selected_subfolders)\n",
        "\n",
        "print(f'Number of images loaded: {len(dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1503799"
      },
      "source": [
        "## Preprocess the images\n",
        "\n",
        "### Subtask:\n",
        "Apply necessary transformations (like ToTensor and normalization) and flatten the images into a 1D vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e82f126a"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the necessary transformations and flatten the images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d46203ba"
      },
      "outputs": [],
      "source": [
        "# Define the transformation pipeline\n",
        "mean = 0.02739770549039046\n",
        "std = 0.09120287481281493\n",
        "\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1)) # Flatten the image tensor\n",
        "])\n",
        "\n",
        "# Update the transform attribute of the dataset\n",
        "dataset.transform = transform_pipeline\n",
        "\n",
        "# Create a list to store the preprocessed and flattened image tensors\n",
        "flattened_images = []\n",
        "\n",
        "# Iterate through the dataset and apply the transformations\n",
        "for img, label in dataset:\n",
        "    flattened_images.append(img)\n",
        "\n",
        "print(f'Number of flattened images: {len(flattened_images)}')\n",
        "# Optionally, check the shape of the first flattened image\n",
        "if flattened_images:\n",
        "    print(f'Shape of the first flattened image: {flattened_images[0].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a21b89c5"
      },
      "source": [
        "## Prepare data for pca\n",
        "\n",
        "### Subtask:\n",
        "Collect the flattened image vectors into a single NumPy array where each row is an image and each column is a pixel feature.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd224176"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the list of flattened image tensors to a NumPy array and print its shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97cdb339"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert the list of flattened image tensors into a single NumPy array\n",
        "images_np = np.array([img.numpy() for img in flattened_images])\n",
        "\n",
        "# Print the shape of the NumPy array\n",
        "print(f'Shape of the NumPy array: {images_np.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddf7371"
      },
      "source": [
        "## Perform pca\n",
        "\n",
        "### Subtask:\n",
        "Apply PCA to the prepared data to reduce dimensionality and find the principal components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "628ad0da"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply PCA to the prepared NumPy array to reduce dimensionality and find the principal components, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3771c8db"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Instantiate PCA\n",
        "# Using a number of components to start with, can be adjusted\n",
        "n_components = 50\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit PCA on the data\n",
        "pca.fit(images_np)\n",
        "\n",
        "# Transform the data\n",
        "images_pca = pca.transform(images_np)\n",
        "\n",
        "# Print the explained variance ratio\n",
        "print(f'Explained variance ratio by each component: {pca.explained_variance_ratio_}')\n",
        "print(f'Total explained variance by {n_components} components: {np.sum(pca.explained_variance_ratio_):.4f}')\n",
        "print(f'Shape of the PCA-transformed data: {images_pca.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f41f1067"
      },
      "source": [
        "## Visualize pca results\n",
        "\n",
        "### Subtask:\n",
        "Plot the images in the reduced-dimensional space (e.g., a 2D scatter plot) and potentially visualize the principal components themselves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff5c25d6"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a scatter plot of the first two principal components, coloring points by class labels, and optionally visualize the principal components as images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d569b4b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the original labels from the dataset\n",
        "labels = [label for _, label in dataset]\n",
        "labels_np = np.array(labels)\n",
        "\n",
        "# Create a scatter plot of the first two principal components\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(images_pca[:, 8], images_pca[:, 49], c=labels_np, cmap='viridis', alpha=0.6)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA of Image Dataset')\n",
        "\n",
        "# Add a legend\n",
        "legend_elements = []\n",
        "for i, class_name in enumerate(dataset.classes):\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', label=class_name,\n",
        "                                      markerfacecolor=plt.cm.viridis(i / (len(dataset.classes) - 1)), markersize=10))\n",
        "\n",
        "plt.legend(handles=legend_elements, title=\"Classes\")\n",
        "plt.show()\n",
        "\n",
        "# Optional: Visualize the first few principal components as images\n",
        "print(\"\\nVisualizing the first few principal components:\")\n",
        "num_components_to_visualize = min(5, n_components) # Visualize up to 5 components\n",
        "\n",
        "# Reshape the principal components back to the original image dimensions\n",
        "# Assuming original image dimensions are 512x512x3 (after converting to RGB)\n",
        "original_shape = (3, 512, 512) # Channels x Height x Width\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(num_components_to_visualize):\n",
        "    pc_image = pca.components_[i].reshape(original_shape)\n",
        "\n",
        "    # For visualization, take the mean across channels or select one channel\n",
        "    pc_image_gray = np.mean(pc_image, axis=0) # Take mean across channels\n",
        "\n",
        "    plt.subplot(1, num_components_to_visualize, i + 1)\n",
        "    plt.imshow(pc_image_gray, cmap='viridis') # Use a suitable colormap\n",
        "    plt.title(f'PC {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9379e1d2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   1200 images were loaded from the specified data directory, classes, and subfolders.\n",
        "*   Each image was successfully transformed using `ToTensor` and `Normalize` and then flattened into a 1D vector of size 786432.\n",
        "*   The flattened images were successfully converted into a NumPy array with a shape of (1200, 786432).\n",
        "*   PCA was applied to the data, reducing the dimensionality to 50 components.\n",
        "*   The first 50 principal components captured approximately 97.36% of the total variance in the dataset.\n",
        "*   A scatter plot of the first two principal components showed the distribution of images in the reduced-dimensional space, colored by their original classes.\n",
        "*   The first few principal components were successfully visualized as images, providing insight into the spatial patterns captured by the PCA.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The PCA analysis effectively reduced the dimensionality of the image data while retaining a high percentage of the variance, suggesting that the first few principal components capture the most significant information.\n",
        "*   The scatter plot of the first two principal components can help assess the separability of different image classes in the reduced-dimensional space, which could inform the choice of subsequent classification or clustering algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f997c186"
      },
      "source": [
        "# Task\n",
        "Train an SVM model to classify images from the dataset located in \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512/\" which is split into subfolders for different classes and sub-subfolders for different data variations. The model should be trained on a subset of the data based on user-selected sub-subfolders and evaluated on a test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a57c66dd"
      },
      "source": [
        "## Load and prepare data for svm\n",
        "\n",
        "### Subtask:\n",
        "Load the image data using the `ProcessedSARImageDataset` class and preprocess it into a format suitable for scikit-learn's SVM. This will likely involve flattening the images and organizing them into a feature matrix and a label vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2908e150"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the data using the `ProcessedSARImageDataset` class, apply the transformations, and prepare the data and labels as NumPy arrays for SVM training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a523feff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Define the data directory path.\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "\n",
        "# 2. Define the list of class names to be included in the dataset.\n",
        "selected_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "\n",
        "# 3. Define the list of subfolder names to be included within each class directory.\n",
        "selected_subfolders = ['_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt']\n",
        "\n",
        "# 4. Instantiate the ProcessedSARImageDataset class with the defined data directory, selected classes, and selected subfolders. Set transform to None initially.\n",
        "dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=selected_classes, transform=None, selected_subfolders=selected_subfolders)\n",
        "\n",
        "# 5. Define a transformation pipeline that includes converting images to tensors, normalizing them using the previously calculated mean and standard deviation (`mean` and `std`), and flattening the tensors into a 1D vector.\n",
        "# The mean and std were calculated in a previous cell (cell_id: VIrQlTU-YGpl and cell_id: 76f043d6).\n",
        "# Using the mean and std from the 3-class experiment with all subfolders\n",
        "mean = 0.02739770549039046\n",
        "std = 0.09120287481281493\n",
        "\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1)) # Flatten the image tensor\n",
        "])\n",
        "\n",
        "# 6. Update the transform attribute of the instantiated dataset object with the defined transformation pipeline.\n",
        "dataset.transform = transform_pipeline\n",
        "\n",
        "# 7. Create empty lists to store the flattened image tensors and their corresponding labels.\n",
        "flattened_images = []\n",
        "labels = []\n",
        "\n",
        "# 8. Iterate through the dataset, apply the transformations to each image, and append the flattened image tensor and its label to the respective lists.\n",
        "for img, label in dataset:\n",
        "    flattened_images.append(img)\n",
        "    labels.append(label)\n",
        "\n",
        "# 9. Convert the list of flattened image tensors into a NumPy array.\n",
        "images_np = np.array([img.numpy() for img in flattened_images])\n",
        "\n",
        "# 10. Convert the list of labels into a NumPy array.\n",
        "labels_np = np.array(labels)\n",
        "\n",
        "print(f'Shape of the image feature matrix: {images_np.shape}')\n",
        "print(f'Shape of the label vector: {labels_np.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d2f2f53"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the data into training and testing sets, then train an SVM classifier on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c481d84"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images_np, labels_np, test_size=0.2, random_state=42, stratify=labels_np)\n",
        "\n",
        "# Instantiate and train the SVM classifier\n",
        "svm_model = SVC(kernel='linear') # Using a linear kernel for simplicity, can be adjusted\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"SVM model trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "965d04e2"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained SVM model on the test set and print the classification report and confusion matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08f05813"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=dataset.classes))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Test Set)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2b36881"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The image data was loaded and preprocessed, resulting in a feature matrix of shape (1200, 786432) and a label vector of shape (1200,).\n",
        "*   The data was split into training and testing sets, with 960 samples for training and 240 samples for testing.\n",
        "*   The trained linear SVM model achieved high classification performance on the test set. The classification report shows precision, recall, and f1-scores of 1.00 for 'Rimossi' and 'Abbattuti', and 0.99 for 'Integri'.\n",
        "*   The confusion matrix indicates only one misclassification on the test set, where an 'Integri' image was predicted as 'Rimossi'.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The linear SVM model performed exceptionally well on this dataset subset. Consider exploring the model's performance on other subsets of the data variations or the full dataset.\n",
        "*   Investigate the single misclassification to understand if there are specific characteristics of that image that led to the incorrect prediction and if any data augmentation or feature engineering could address it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63f97bb0"
      },
      "source": [
        "# Task\n",
        "Implement and evaluate an SVM classifier with cross-validation for the image classification task using the data loaded with `ProcessedSARImageDataset`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1002efe0"
      },
      "source": [
        "## Load and prepare data for svm\n",
        "\n",
        "### Subtask:\n",
        "Load the image data using the `ProcessedSARImageDataset` class and preprocess it into a format suitable for scikit-learn's SVM. This will likely involve flattening the images and organizing them into a feature matrix and a label vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76979069"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the data using the ProcessedSARImageDataset class, apply the transformations, and prepare the data and labels as NumPy arrays for SVM training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5e99ba0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Define the data directory path.\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "\n",
        "# 2. Define the list of class names to be included in the dataset.\n",
        "selected_classes = ['Integri', 'Rimossi']\n",
        "\n",
        "# 3. Define the list of subfolder names to be included within each class directory.\n",
        "selected_subfolders = ['_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt']\n",
        "\n",
        "# 4. Instantiate the ProcessedSARImageDataset class with the defined data directory, selected classes, and selected subfolders. Set transform to None initially.\n",
        "dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=selected_classes, transform=None, selected_subfolders=selected_subfolders)\n",
        "\n",
        "# 5. Define a transformation pipeline that includes converting images to tensors, normalizing them using the previously calculated mean and standard deviation (`mean` and `std`), and flattening the tensors into a 1D vector.\n",
        "# The mean and std were calculated in a previous cell (cell_id: VIrQlTU-YGpl and cell_id: 76f043d6).\n",
        "# Using the mean and std from the 3-class experiment with all subfolders\n",
        "mean = 0.02739770549039046\n",
        "std = 0.09120287481281493\n",
        "\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1)) # Flatten the image tensor\n",
        "])\n",
        "\n",
        "# 6. Update the transform attribute of the instantiated dataset object with the defined transformation pipeline.\n",
        "dataset.transform = transform_pipeline\n",
        "\n",
        "# 7. Create empty lists to store the flattened image tensors and their corresponding labels.\n",
        "flattened_images = []\n",
        "labels = []\n",
        "\n",
        "# 8. Iterate through the dataset, apply the transformations to each image, and append the flattened image tensor and its label to the respective lists.\n",
        "for img, label in dataset:\n",
        "    flattened_images.append(img)\n",
        "    labels.append(label)\n",
        "\n",
        "# 9. Convert the list of flattened image tensors into a NumPy array.\n",
        "images_np = np.array([img.numpy() for img in flattened_images])\n",
        "\n",
        "# 10. Convert the list of labels into a NumPy array.\n",
        "labels_np = np.array(labels)\n",
        "\n",
        "# 11. Print the shapes of the image feature matrix and the label vector to verify the data preparation.\n",
        "print(f'Shape of the image feature matrix: {images_np.shape}')\n",
        "print(f'Shape of the label vector: {labels_np.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4991401"
      },
      "source": [
        "## Split data\n",
        "\n",
        "### Subtask:\n",
        "Split the prepared data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122cd48b"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the prepared data into training and testing sets using train_test_split, ensuring stratification and setting a random_state for reproducibility, and then print the shapes of the resulting arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7c31e6f"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images_np, labels_np, test_size=0.2, random_state=42, stratify=labels_np)\n",
        "\n",
        "# Print the shapes of the resulting training and testing sets\n",
        "print(f'Shape of X_train: {X_train.shape}')\n",
        "print(f'Shape of X_test: {X_test.shape}')\n",
        "print(f'Shape of y_train: {y_train.shape}')\n",
        "print(f'Shape of y_test: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "996bbc8a"
      },
      "source": [
        "## Define svm model\n",
        "\n",
        "### Subtask:\n",
        "Instantiate an SVM classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f71ea69"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the SVC class and instantiate the SVM model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bffc278"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Instantiate the SVM classifier\n",
        "svm_model = SVC(kernel='linear', random_state=42) # Using a linear kernel and setting random_state for reproducibility\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"SVM model instantiated successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5caaf6ae"
      },
      "source": [
        "## Implement cross-validation\n",
        "\n",
        "### Subtask:\n",
        "Perform cross-validation on the training data to evaluate the model's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29689807"
      },
      "source": [
        "**Reasoning**:\n",
        "Perform cross-validation on the training data to evaluate the model's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "956d7def"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation on the training data\n",
        "cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "\n",
        "# Print the mean and standard deviation of the cross-validation scores\n",
        "print(f\"Mean cross-validation accuracy: {np.mean(cv_scores):.4f}\")\n",
        "print(f\"Standard deviation of cross-validation accuracy: {np.std(cv_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08O0fIRiXLIk"
      },
      "outputs": [],
      "source": [
        "# Train the SVM classifier\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=dataset.classes))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Test Set)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54d34f66"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The image data was successfully loaded and preprocessed, resulting in a feature matrix of shape (1200, 786432) and a label vector of shape (1200,).\n",
        "*   The data was split into training (960 samples) and testing (240 samples) sets with a 80/20 ratio, maintaining the class distribution through stratification.\n",
        "*   Cross-validation on the training data with 5 folds yielded a mean accuracy of 0.9984 and a standard deviation of 0.0031, indicating high and consistent performance.\n",
        "*   The SVM model, after training on the training data, achieved perfect precision, recall, f1-score, and accuracy (1.00) on the test set.\n",
        "*   The confusion matrix confirmed the perfect classification on the test set, with all 80 samples of 'Integri' and all 80 samples of 'Rimossi' being correctly predicted.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current SVM model with a linear kernel performs exceptionally well on this specific dataset and class subset.\n",
        "*   Explore alternative kernels (e.g., RBF) and hyperparameters for the SVM model to see if performance can be maintained or improved with potentially lower complexity or better generalization to unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from skimage.feature import hog, local_binary_pattern\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "#Load the dataset\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/ProcessedDataset_512\"\n",
        "\n",
        "# Define the class lists\n",
        "two_classes = ['Integri', 'Abbattuti']\n",
        "three_classes = ['Integri', 'Rimossi', 'Abbattuti']\n",
        "selected_subfolders = ['_0', '_3x20_NoExtraTgt']#, '_5x25_NoExtraTgt'] # Updated subfolder names\n",
        "\n",
        "\n",
        "# Create the datasets first, before normalization\n",
        "two_class_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=two_classes, transform=transforms.ToTensor(),  selected_subfolders=selected_subfolders)\n",
        "three_class_dataset = ProcessedSARImageDataset(root_dir=data_dir, selected_classes=three_classes, transform=transforms.ToTensor(),  selected_subfolders=selected_subfolders)\n",
        "\n",
        "\n",
        "# Choose the dataset (either two_class_dataset or three_class_dataset)\n",
        "dataset_to_use = two_class_dataset  # Or three_class_dataset\n",
        "\n",
        "# Define the model based on the number of classes in the chosen dataset\n",
        "num_classes = len(dataset_to_use.classes)\n",
        "\n",
        "\n",
        "# -------- HOG / LBP helpers --------\n",
        "def compute_hog_per_channel(img_ch,\n",
        "                            pixels_per_cell=(8,8),\n",
        "                            cells_per_block=(2,2),\n",
        "                            orientations=9,\n",
        "                            block_norm=\"L2-Hys\"):\n",
        "    \"\"\"\n",
        "    img_ch: 2D numpy array (H, W) float in [0,1]\n",
        "    returns: 1D hog feature vector for this channel\n",
        "    \"\"\"\n",
        "    return hog(img_ch,\n",
        "               orientations=orientations,\n",
        "               pixels_per_cell=pixels_per_cell,\n",
        "               cells_per_block=cells_per_block,\n",
        "               block_norm=block_norm,\n",
        "               visualize=False,\n",
        "               feature_vector=True)\n",
        "\n",
        "def compute_lbp_hist_per_channel(img_ch, P=8, R=1, method=\"uniform\", normalize=True):\n",
        "    \"\"\"\n",
        "    img_ch: 2D numpy array (H, W) float in [0,1]\n",
        "    returns: normalized histogram of LBP codes for this channel\n",
        "    \"\"\"\n",
        "    lbp = local_binary_pattern(img_ch, P=P, R=R, method=method)\n",
        "    # 'uniform' bins = P+2\n",
        "    n_bins = P + 2 if method == \"uniform\" else int(lbp.max() + 1)\n",
        "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=False)\n",
        "    if normalize:\n",
        "        hist = hist.astype(np.float32)\n",
        "        s = hist.sum()\n",
        "        if s > 0:\n",
        "            hist /= s\n",
        "    return hist\n",
        "\n",
        "def tensor_to_numpy(img_tensor):\n",
        "    \"\"\"\n",
        "    img_tensor: torch.Tensor shaped (C, H, W), dtype float, range [0,1]\n",
        "    returns: numpy array (C, H, W) float32\n",
        "    \"\"\"\n",
        "    arr = img_tensor.detach().cpu().numpy().astype(np.float32)\n",
        "    # If any NaNs/Infs slipped in, clean:\n",
        "    arr = np.nan_to_num(arr, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "    return arr\n",
        "\n",
        "def extract_features_from_dataset(dataset,\n",
        "                                  hog_ppc=(8,8),\n",
        "                                  hog_cpb=(2,2),\n",
        "                                  hog_orient=9,\n",
        "                                  lbp_P=8,\n",
        "                                  lbp_R=1,\n",
        "                                  lbp_method=\"uniform\",\n",
        "                                  standardize=True,\n",
        "                                  save_path=None):\n",
        "    \"\"\"\n",
        "    Iterates over the dataset (no DataLoader needed) and computes\n",
        "    HOG and LBP features per channel; concatenates channels.\n",
        "    Returns: X_hog, X_lbp, X_all, y, (scaler if standardize else None)\n",
        "    \"\"\"\n",
        "    X_hog = []\n",
        "    X_lbp = []\n",
        "    y = []\n",
        "\n",
        "    for i in tqdm(range(len(dataset)), desc=\"Extracting HOG/LBP\"):\n",
        "        img_t, label = dataset[i]          # img_t: torch.Tensor (C,H,W)\n",
        "        arr = tensor_to_numpy(img_t)       # (C,H,W)\n",
        "        C, H, W = arr.shape\n",
        "\n",
        "        # Per-channel HOG + LBP, then concat\n",
        "        hog_feats = []\n",
        "        lbp_feats = []\n",
        "        for c in range(C):\n",
        "            ch = arr[c]\n",
        "            hog_vec = compute_hog_per_channel(\n",
        "                ch,\n",
        "                pixels_per_cell=hog_ppc,\n",
        "                cells_per_block=hog_cpb,\n",
        "                orientations=hog_orient\n",
        "            )\n",
        "            lbp_hist = compute_lbp_hist_per_channel(\n",
        "                ch,\n",
        "                P=lbp_P,\n",
        "                R=lbp_R,\n",
        "                method=lbp_method,\n",
        "                normalize=True\n",
        "            )\n",
        "            hog_feats.append(hog_vec)\n",
        "            lbp_feats.append(lbp_hist)\n",
        "\n",
        "        hog_feats = np.concatenate(hog_feats, axis=0)\n",
        "        lbp_feats = np.concatenate(lbp_feats, axis=0)\n",
        "\n",
        "        X_hog.append(hog_feats)\n",
        "        X_lbp.append(lbp_feats)\n",
        "        y.append(label)\n",
        "\n",
        "    X_hog = np.vstack(X_hog).astype(np.float32)\n",
        "    X_lbp = np.vstack(X_lbp).astype(np.float32)\n",
        "    y = np.array(y, dtype=np.int64)\n",
        "    X_all = np.hstack([X_hog, X_lbp]).astype(np.float32)\n",
        "\n",
        "    scaler = None\n",
        "    if standardize:\n",
        "        scaler = StandardScaler(with_mean=False)  # sparse-friendly; HOG/LBP are nonnegative\n",
        "        X_all = scaler.fit_transform(X_all)\n",
        "\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        np.savez_compressed(save_path, X_hog=X_hog, X_lbp=X_lbp, X_all=X_all, y=y)\n",
        "        if scaler is not None:\n",
        "            joblib.dump(scaler, save_path + \".scaler.joblib\")\n",
        "        print(f\"Saved features to: {save_path} (+ scaler if used)\")\n",
        "\n",
        "    return X_hog, X_lbp, X_all, y, scaler\n",
        "\n",
        "# -------- Run it on your chosen dataset_to_use --------\n",
        "# Tip: if your dataset is large, you could also iterate via a DataLoader,\n",
        "# but here we read directly to respect your custom __getitem__ logic.\n",
        "X_hog, X_lbp, X_all, y, scaler = extract_features_from_dataset(\n",
        "    dataset_to_use,\n",
        "    hog_ppc=(16,16),\n",
        "    hog_cpb=(2,2),\n",
        "    hog_orient=9,\n",
        "    lbp_P=48,\n",
        "    lbp_R=1,\n",
        "    lbp_method=\"uniform\",\n",
        "    standardize=True,\n",
        "    save_path=\"/content/drive/MyDrive/Colab Notebooks/Towercheck/features/towercheck_hog_lbp.npz\"\n",
        ")\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"X_hog:\", X_hog.shape)\n",
        "print(\"X_lbp:\", X_lbp.shape)\n",
        "print(\"X_all:\", X_all.shape)\n",
        "print(\"y:\", y.shape)\n"
      ],
      "metadata": {
        "id": "3wmDe52P6DUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iYtC5Dgg-Ode"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Stratified K-Fold CV on saved HOG+LBP features ======\n",
        "# Requirements: scikit-learn, numpy\n",
        "# pip install scikit-learn numpy\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# ---------- 1) Load saved features ----------\n",
        "npz_path = \"/content/drive/MyDrive/Colab Notebooks/Towercheck/features/towercheck_hog_lbp.npz\"\n",
        "if not os.path.exists(npz_path):\n",
        "    raise FileNotFoundError(f\"NPZ not found at: {npz_path}\")\n",
        "\n",
        "data = np.load(npz_path)\n",
        "X_hog = data[\"X_hog\"]\n",
        "X_lbp = data[\"X_lbp\"]\n",
        "X_all = data[\"X_all\"]\n",
        "y     = data[\"y\"]\n",
        "\n",
        "print(\"Loaded shapes:\")\n",
        "print(\"  X_hog:\", X_hog.shape)\n",
        "print(\"  X_lbp:\", X_lbp.shape)\n",
        "print(\"  X_all:\", X_all.shape)\n",
        "print(\"  y:\", y.shape)\n",
        "\n",
        "# ---------- 2) Choose feature matrix ----------\n",
        "# Options: X_all (HOG+LBP), X_hog (HOG only), X_lbp (LBP only)\n",
        "X = X_lbp\n",
        "\n",
        "# ---------- 3) Class names (auto for 2/3 classes; customize if needed) ----------\n",
        "n_classes = len(np.unique(y))\n",
        "if n_classes == 2:\n",
        "    # Adjust if your label order differs\n",
        "    class_names = [\"Integri\", \"Abbattuti\"]\n",
        "elif n_classes == 3:\n",
        "    class_names = [\"Integri\", \"Rimossi\", \"Abbattuti\"]\n",
        "else:\n",
        "    class_names = [f\"class_{i}\" for i in range(n_classes)]\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# ---------- 4) Define CV and model ----------\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=None,          # set e.g. 60 if you see overfitting\n",
        "    max_features=\"sqrt\",\n",
        "    class_weight=\"balanced\", # helpful if classes are imbalanced\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------- 5) Cross-validate with multiple metrics ----------\n",
        "scoring = {\n",
        "    \"acc\": \"accuracy\",\n",
        "    \"f1_macro\": \"f1_macro\",\n",
        "    \"f1_weighted\": \"f1_weighted\",\n",
        "}\n",
        "\n",
        "cv_res = cross_validate(\n",
        "    rf, X, y,\n",
        "    cv=cv,\n",
        "    scoring=scoring,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "print(\"\\nPer-fold metrics:\")\n",
        "print(\"  Accuracy   :\", np.round(cv_res[\"test_acc\"], 4))\n",
        "print(\"  Macro-F1   :\", np.round(cv_res[\"test_f1_macro\"], 4))\n",
        "print(\"  Weighted-F1:\", np.round(cv_res[\"test_f1_weighted\"], 4))\n",
        "\n",
        "print(\"\\nAveraged metrics (mean ± std):\")\n",
        "print(f\"  Accuracy    : {cv_res['test_acc'].mean():.4f} ± {cv_res['test_acc'].std():.4f}\")\n",
        "print(f\"  Macro-F1    : {cv_res['test_f1_macro'].mean():.4f} ± {cv_res['test_f1_macro'].std():.4f}\")\n",
        "print(f\"  Weighted-F1 : {cv_res['test_f1_weighted'].mean():.4f} ± {cv_res['test_f1_weighted'].std():.4f}\")\n",
        "\n",
        "# ---------- 6) Out-of-fold predictions for a single overall report ----------\n",
        "# Each sample is predicted by a model that did NOT train on it.\n",
        "y_oof = cross_val_predict(rf, X, y, cv=cv, n_jobs=-1)\n",
        "\n",
        "print(\"\\n=== Out-of-fold performance (aggregated) ===\")\n",
        "print(f\"OOF Accuracy: {accuracy_score(y, y_oof):.4f}\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nClassification report (OOF):\")\n",
        "    print(classification_report(y, y_oof, target_names=class_names, digits=4))\n",
        "except Exception:\n",
        "    # Fallback in case class_names length/order doesn't match y's labels\n",
        "    print(\"\\nClassification report (OOF):\")\n",
        "    print(classification_report(y, y_oof, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y, y_oof)\n",
        "print(\"Confusion matrix (OOF):\\n\", cm)\n",
        "\n",
        "# ---------- 7) (Optional) Fit on full data and save model ----------\n",
        "# from joblib import dump\n",
        "# rf.fit(X, y)\n",
        "# dump(rf, \"/content/drive/MyDrive/Colab Notebooks/Towercheck/models/rf_cv_final.joblib\")\n",
        "# print(\"Saved final RF model.\")\n"
      ],
      "metadata": {
        "id": "KFfqP5meEVH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Visualize HOG & LBP: one example per class ===\n",
        "# Works with your existing `dataset_to_use` (expects (C,H,W) tensors, 2 channels OK)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import hog, local_binary_pattern\n",
        "\n",
        "# ---- Params (tweak as you like) ----\n",
        "CHANNEL = 0              # which channel to visualize\n",
        "P, R = 48, 1              # LBP neighbors & radius\n",
        "LBP_METHOD = \"uniform\"   # \"uniform\" keeps bins = P+2\n",
        "ORIENT = 9               # HOG orientations\n",
        "PPC = (16,16)            # HOG pixels per cell (you chose 16x16)\n",
        "CPB = (2,2)              # HOG cells per block\n",
        "BLOCK_NORM = \"L2-Hys\"\n",
        "SEED = 0                 # for reproducible sampling\n",
        "\n",
        "# ---- Pick one index per class (random but reproducible) ----\n",
        "rng = np.random.default_rng()\n",
        "labels_seen = {}\n",
        "idxs = np.arange(len(dataset_to_use))\n",
        "rng.shuffle(idxs)\n",
        "\n",
        "class_names = getattr(dataset_to_use, \"classes\", None)\n",
        "n_classes = len(class_names) if class_names is not None else None\n",
        "\n",
        "for i in idxs:\n",
        "    _, lab = dataset_to_use[i]\n",
        "    lab = int(lab)\n",
        "    if lab not in labels_seen:\n",
        "        labels_seen[lab] = i\n",
        "        if n_classes is not None and len(labels_seen) == n_classes:\n",
        "            break\n",
        "\n",
        "# If class names are missing, fall back to numeric strings\n",
        "if class_names is None:\n",
        "    class_names = [f\"class_{k}\" for k in sorted(labels_seen.keys())]\n",
        "\n",
        "# ---- Plot: one row per class, 4 columns ----\n",
        "rows = len(labels_seen)\n",
        "cols = 4\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
        "if rows == 1:\n",
        "    axes = axes[None, :]  # make 2D for consistent indexing\n",
        "\n",
        "for r, lab in enumerate(sorted(labels_seen.keys())):\n",
        "    idx = labels_seen[lab]\n",
        "    img_t, _ = dataset_to_use[idx]     # torch tensor (C,H,W)\n",
        "    arr = img_t.numpy()\n",
        "    ch = arr[min(CHANNEL, arr.shape[0]-1)]\n",
        "\n",
        "    # HOG (with visualization)\n",
        "    hog_vec, hog_img = hog(\n",
        "        ch,\n",
        "        orientations=ORIENT,\n",
        "        pixels_per_cell=PPC,\n",
        "        cells_per_block=CPB,\n",
        "        visualize=True,\n",
        "        feature_vector=True,\n",
        "        block_norm=BLOCK_NORM\n",
        "    )\n",
        "\n",
        "    # LBP map + histogram\n",
        "    lbp = local_binary_pattern(ch, P=P, R=R, method=LBP_METHOD)\n",
        "    n_bins = P + 2 if LBP_METHOD == \"uniform\" else int(lbp.max() + 1)\n",
        "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
        "\n",
        "    # ----- Plots for this class -----\n",
        "    axes[r, 0].imshow(ch, cmap=\"gray\")\n",
        "    axes[r, 0].set_title(f\"{class_names[lab]}  (idx={idx})\")\n",
        "    axes[r, 0].axis(\"off\")\n",
        "\n",
        "    axes[r, 1].imshow(hog_img, cmap=\"gray\")\n",
        "    axes[r, 1].set_title(\"HOG visualization\")\n",
        "    axes[r, 1].axis(\"off\")\n",
        "\n",
        "    im = axes[r, 2].imshow(lbp, cmap=\"magma\")\n",
        "    axes[r, 2].set_title(f\"LBP code map (P={P}, R={R})\")\n",
        "    axes[r, 2].axis(\"off\")\n",
        "\n",
        "    axes[r, 3].bar(np.arange(n_bins), hist, width=0.9)\n",
        "    axes[r, 3].set_title(\"LBP histogram\")\n",
        "    axes[r, 3].set_xlabel(\"LBP code\")\n",
        "    axes[r, 3].set_ylabel(\"frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ODH-zMNWKpV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Total intensity distribution per class (histograms only, counts) ===\n",
        "# Uses dataset_to_use (expects (C,H,W) tensors with >=2 channels)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- CONFIG ----\n",
        "USE_SQRT   = True   # True: sum sqrt(ch1^2+ch2^2); False: sum (ch1^2+ch2^2)\n",
        "LOG_XAXIS  = False  # True -> log-scaled x-axis with log-spaced bins\n",
        "BINS_COUNT = 40\n",
        "CHANNELS   = (0, 1) # which two channels to use\n",
        "\n",
        "def collect_total_intensities(dataset, use_sqrt=True, ch_idx=(0,1)):\n",
        "    vals = defaultdict(list)\n",
        "    for i in tqdm(range(len(dataset)), desc=\"Total intensities\"):\n",
        "        img, lab = dataset[i]  # (C,H,W)\n",
        "        ch1 = img[min(ch_idx[0], img.shape[0]-1)].float()\n",
        "        ch2 = img[min(ch_idx[1], img.shape[0]-1)].float()\n",
        "        mag = torch.sqrt(ch1**2 + ch2**2) if use_sqrt else (ch1**2 + ch2**2)\n",
        "        vals[int(lab)].append(mag.sum().item())\n",
        "    return {k: np.asarray(v, dtype=np.float64) for k, v in vals.items()}\n",
        "\n",
        "# ---- Compute totals ----\n",
        "totals = collect_total_intensities(dataset_to_use, use_sqrt=USE_SQRT, ch_idx=CHANNELS)\n",
        "\n",
        "# ---- Class labels ----\n",
        "class_names = getattr(dataset_to_use, \"classes\", None)\n",
        "keys = sorted(totals.keys())\n",
        "labels = [class_names[k] if (class_names and k < len(class_names)) else str(k) for k in keys]\n",
        "\n",
        "# ---- Quick summary (optional) ----\n",
        "print(\"Counts per class:\", {labels[i]: totals[k].size for i,k in enumerate(keys)})\n",
        "for lab, k in zip(labels, keys):\n",
        "    arr = totals[k]\n",
        "    print(f\"{lab:>12s}: mean={arr.mean():.6f}, std={arr.std(ddof=1) if arr.size>1 else 0.0:.6f}, \"\n",
        "          f\"min={arr.min():.6f}, max={arr.max():.6f}\")\n",
        "\n",
        "# ---- Plot: Overlaid histograms (counts) ----\n",
        "plt.figure(figsize=(7,4))\n",
        "all_vals = np.concatenate([totals[k] for k in keys])\n",
        "vmin, vmax = float(all_vals.min()), float(all_vals.max())\n",
        "if vmax <= vmin:  # guard against degenerate ranges\n",
        "    vmax = vmin + 1e-9\n",
        "\n",
        "if LOG_XAXIS:\n",
        "    vmin = max(vmin, 1e-12)\n",
        "    bins = np.logspace(np.log10(vmin), np.log10(vmax), BINS_COUNT)\n",
        "    for k, lab in zip(keys, labels):\n",
        "        plt.hist(totals[k], bins=bins, density=False, alpha=0.5, label=lab)\n",
        "    plt.xscale(\"log\")\n",
        "    plt.xlabel(\"Total intensity (log scale)\")\n",
        "else:\n",
        "    bins = np.linspace(vmin, vmax, BINS_COUNT)\n",
        "    for k, lab in zip(keys, labels):\n",
        "        plt.hist(totals[k], bins=bins, density=False, alpha=0.5, label=lab)\n",
        "    plt.xlabel(\"Total intensity\")\n",
        "\n",
        "plt.ylabel(\"Count\")\n",
        "#plt.title(\"Per-class total intensity — histograms\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vuAGTzIgPjP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38f9d2e0"
      },
      "source": [
        "# Task\n",
        "Plot the confusion matrix for the test set, and then plot four additional confusion matrices, each considering only the images contained in the following subfolders: '_0', '_3x20_NoExtraTgt', '_5x25_NoExtraTgt', '_5x25_WithExtraTgt_Cls'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec761508"
      },
      "source": [
        "## Identify test set images by subfolder\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the `test_dataset` to identify which images belong to each subfolder. Store the indices and labels for images from each subfolder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93abef30"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the test dataset to identify which images belong to each subfolder and store their indices and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6bab10c"
      },
      "source": [
        "# Create dictionaries to store indices and labels per subfolder\n",
        "subfolder_indices = {subfolder: [] for subfolder in selected_subfolders + ['_5x25_WithExtraTgt_Cls']}\n",
        "subfolder_labels = {subfolder: [] for subfolder in selected_subfolders + ['_5x25_WithExtraTgt_Cls']}\n",
        "\n",
        "# Iterate through the test_dataset using its indices\n",
        "for i in test_dataset.indices:\n",
        "    # Get the image path from the original dataset\n",
        "    image_path = dataset_to_use.image_paths[i]\n",
        "    true_label = dataset_to_use.labels[i]\n",
        "\n",
        "    # Determine the subfolder name from the image path\n",
        "    found_subfolder = None\n",
        "    # Include the additional subfolder for evaluation\n",
        "    all_subfolders_to_check = selected_subfolders + ['_5x25_WithExtraTgt_Cls']\n",
        "    for subfolder in all_subfolders_to_check:\n",
        "        # Check if the subfolder name is present in the image path\n",
        "        if subfolder in image_path:\n",
        "            found_subfolder = subfolder\n",
        "            break\n",
        "\n",
        "    # If a subfolder is identified, store the index and label\n",
        "    if found_subfolder:\n",
        "        subfolder_indices[found_subfolder].append(i)\n",
        "        subfolder_labels[found_subfolder].append(true_label)\n",
        "    else:\n",
        "        # Handle cases where an image might not belong to any of the specified subfolders\n",
        "        print(f\"Warning: Image path {image_path} does not belong to any of the specified subfolders.\")\n",
        "\n",
        "\n",
        "# Print the number of images found for each subfolder\n",
        "print(\"Number of images per subfolder in the test set:\")\n",
        "for subfolder, indices in subfolder_indices.items():\n",
        "    print(f\"  {subfolder}: {len(indices)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMQSe9ajhOkZWHsazVX89UT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}